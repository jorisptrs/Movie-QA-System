{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Question Analysis\n",
    "\n",
    "The goal of this assignment is to write a more flexible version of the interactive QA system. As in the previous assignment, the system should be able to take a question in natural language (English) as input, analyse the question, and generate a SPARQL query for it.\n",
    "\n",
    "## Assignment  // Additional requirements\n",
    "\n",
    "* Make sure that your system can analyse at least two more question types. E.g. questions that start with *which*, *when*, where the property is expressed by a verb, etc.\n",
    "* Apart from the techniques introduced last week (matching tokens on the basis of their lemma or part-of-speech), also include at least one pattern where you use the dependency relations to find the relevant property or entity in the question. \n",
    "* Include 10 examples of questions that your system can handle, and that illustrate the fact that you cover additional question types\n",
    "\n",
    "## Examples\n",
    "\n",
    "Here is a non-representative list of questios and question types to consider. See the list with all questions for more examples\n",
    "\n",
    "* For what movie did Leonardo DiCaprio win an Oscar?\n",
    "* How long is Pulp Fiction?\n",
    "* How many episodes does Twin Peaks have?\n",
    "* In what capital was the film The Fault in Our Stars, filmed?\n",
    "* In what year was The Matrix released?\n",
    "* When did Alan Rickman die?\n",
    "* Where was Morgan Freeman born?\n",
    "* Which actor played Aragorn in Lord of the Rings?\n",
    "* Which actors played the role of James Bond\n",
    "* Who directed The Shawshank Redemption?\n",
    "* Which movies are directed by Alice Wu?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_trf') # this loads the model for analysing English text\n",
    "                   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment Submission\n",
    "### SRECK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code from last assignment\n",
    "- Get wikidata IDs\n",
    "- Generate SPARQL Queries\n",
    "- Connect to wikidata endpoint to get SPARQL results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Query helpers\n",
    "\"\"\"\n",
    "\n",
    "import requests\n",
    "\n",
    "def reduce_based_on_ids(id_list):\n",
    "    \"\"\"\n",
    "    If there are multiple ways of getting a list of properties,\n",
    "    then they may be repeated. This simply removes duplicates,\n",
    "    while not changing the relative order within the input list.\n",
    "    \"\"\"\n",
    "    id_set = {}\n",
    "    for obj in id_list:\n",
    "        id_set[obj['id']] = obj\n",
    "\n",
    "    return list(id_set.values())\n",
    "\n",
    "def get_wikidata_ids_of_word(name, search_property = False):\n",
    "    \"\"\"\n",
    "    Returns a list of ID dictionaries (with labels and possibly descriptions)\n",
    "    for a given name, either looking for entities or properties (set search_property:=True for the latter)\n",
    "    Each dict contains keys: 'id', 'label', and possibly 'description'.\n",
    "    If a description cannot be found, it will not be included in the dict.\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "    \n",
    "    url = 'https://www.wikidata.org/w/api.php'\n",
    "    params = {'action':'wbsearchentities', \n",
    "              'language':'en',\n",
    "              'format':'json'}\n",
    "    \n",
    "    # add a param to the request if it needs to look for a property\n",
    "    if search_property:\n",
    "        params['type'] = 'property'\n",
    "    \n",
    "    params['search'] = name\n",
    "    json = requests.get(url,params).json()\n",
    "    \n",
    "    # extract only the useful data from the json file\n",
    "    try:\n",
    "        for result in json['search']:\n",
    "            # append an empty dictionary\n",
    "            all_results.append({})\n",
    "            # add the ID and label\n",
    "            all_results[-1]['id'] = result['id']\n",
    "            all_results[-1]['label'] = result['label']\n",
    "            # add a description if it exists\n",
    "            if 'description' in result.keys():\n",
    "                all_results[-1]['description'] = result['description']\n",
    "    except Exception:\n",
    "        # no results\n",
    "        pass\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "def get_wikidata_ids(list_of_words, search_property = False):\n",
    "    \"\"\"\n",
    "    Returns a set of candidate id's for the list of words\n",
    "    \"\"\"\n",
    "    list_of_ids = []\n",
    "    for word in list_of_words:\n",
    "        list_of_ids += get_wikidata_ids_of_word(word, search_property)\n",
    "    # remove duplicates\n",
    "    set_of_ids = reduce_based_on_ids(list_of_ids)\n",
    "    return set_of_ids\n",
    "\n",
    "def simple_sparql_query(entity_id, property_id, entity_id_2 = None, reverse = False, binary = False):\n",
    "    \"\"\" \n",
    "    Returns string with entity id and property id in place as a SPARQL query\n",
    "    \"\"\"\n",
    "    if reverse:\n",
    "        p2 = \"wd:\" + entity_id\n",
    "        p1 = \"?answer\"\n",
    "    else:\n",
    "        p1 = \"wd:\" + entity_id\n",
    "        p2 = \"?answer\"\n",
    "        \n",
    "    if binary:\n",
    "        query = f'''ASK {{\n",
    "            wd:{entity_id} wdt:{property_id} wd:{entity_id_2} .\n",
    "        }}'''\n",
    "    else:\n",
    "        query = f'''SELECT ?answerLabel WHERE {{\n",
    "            {p1} wdt:{property_id} {p2}.\n",
    "            SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
    "        }}'''\n",
    "    return query\n",
    "\n",
    "def property_qualifier_query(entity_id, property_id, qualifierEntity_id, qualifierProperty_id, reverse) :\n",
    "    # Find answer to the original property with qualifier property as filter\n",
    "    if reverse:\n",
    "        p2 = \"wd:\" + qualifierEntity_id\n",
    "        p1 = \"?item\"\n",
    "    else:\n",
    "        p1 = \"wd:\" + qualifierEntity_id\n",
    "        p2 = \"?item\"\n",
    "    query = f'''SELECT ?itemLabel WHERE {{ \n",
    "        wd:{entity_id} p:{property_id} ?stat . \n",
    "        ?stat ps:{property_id} {p1} . \n",
    "        ?stat pq:{qualifierProperty_id} {p2} .\n",
    "        SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
    "      }}'''\n",
    "    return query\n",
    "\n",
    "def simple_qualifier_query(entity_id, property_id) :\n",
    "    query = f'''SELECT ?itemLabel WHERE {{ \n",
    "        wd:{entity_id} p:{property_id} ?stat . \n",
    "        ?stat ps:{property_id} ?item .\n",
    "        SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
    "      }}'''\n",
    "    return query\n",
    "\n",
    "#Possibly ask query with qualifier?\n",
    "\n",
    "def get_SPARQL_results(query, shouldBeCounted = False):\n",
    "    \"\"\"\n",
    "    Relates to previous assignment. Return results (string) for a SPARQL query.\n",
    "    The format is arbitrary can can be changed as desired.\n",
    "    \"\"\"\n",
    "    url = 'https://query.wikidata.org/sparql'\n",
    "    if shouldBeCounted:\n",
    "        result = 0\n",
    "    else:\n",
    "        result = \"\"\n",
    "    # Max 1000 attempts\n",
    "    for _ in range(1000):\n",
    "        data = requests.get(url, params={'query': query, 'format': 'json'})\n",
    "        if data.status_code == 200:\n",
    "            break\n",
    "    data = data.json()\n",
    "    try:\n",
    "        return data['boolean']\n",
    "    except:\n",
    "        for item in data['results']['bindings']:\n",
    "            for var in item:\n",
    "                if shouldBeCounted:\n",
    "                    result += 1\n",
    "                else:\n",
    "                    result += ('{}\\t{}\\n'.format(var,item[var]['value']))\n",
    "    \n",
    "    return str(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Linguistic Helpers\n",
    "\"\"\"\n",
    "\n",
    "def get_root(doc):\n",
    "    \"\"\"\n",
    "    Return the root of the dependency tree\n",
    "    in a given nlp-parsed sentence (root)\n",
    "    \"\"\"\n",
    "    for word in doc:\n",
    "        if word.dep_ == \"ROOT\":\n",
    "            return word\n",
    "        \n",
    "def is_q_word(string):\n",
    "    return string in ['who', 'what', 'which', 'how', 'when', 'where', 'why', 'whom']\n",
    "                      \n",
    "def is_exception(string):\n",
    "    return string in ['was', 'is', 'does', 'did'] or is_q_word(string)\n",
    "        \n",
    "def phrase(word, remove = \"\"):\n",
    "    \"\"\"\n",
    "    Given code: Return the phrase that the given word heads\n",
    "    \"\"\"\n",
    "    children = []\n",
    "    for child in word.subtree:\n",
    "        children.append(child.text.replace(remove,''))\n",
    "    return \" \".join(children)\n",
    "\n",
    "def check_key_words(doc):\n",
    "    \"\"\"\n",
    "    Check for special question words and other keywords\n",
    "    \"\"\"\n",
    "    key_words = {\n",
    "        'how many' : 'number',\n",
    "        'quantity' : 'number',\n",
    "        'amount' : 'number',\n",
    "        'number of': 'number',\n",
    "        'how long' : 'duration',\n",
    "        'how often' : 'frequency',\n",
    "        'when' : 'date',\n",
    "        'where' : 'place',\n",
    "        'why' : 'cause',\n",
    "        'whose' : 'owner',\n",
    "        'birthday' : 'date of birth',\n",
    "        'directed' : 'director'\n",
    "    }\n",
    "    for i in range(len(doc)-1):\n",
    "        one_word = doc[i].text.lower()\n",
    "        two_words = doc[i].text.lower() + \" \" + doc[i+1].text.lower()\n",
    "        if two_words in list(key_words.keys()):\n",
    "            return key_words[two_words]\n",
    "        elif one_word in list(key_words.keys()):\n",
    "            return key_words[one_word]\n",
    "    return None\n",
    "\n",
    "def add_variations(prop):\n",
    "    if prop == 'born' or prop == 'bear':\n",
    "        return ['birth place', 'birth date']\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "def remove_duplicates(ls):\n",
    "    d = {}\n",
    "    for l in ls:\n",
    "        d[l] = 0\n",
    "    return list(d.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Entity extraction functions ###\n",
    "import re\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "\n",
    "def get_named_entities(doc):\n",
    "    \"\"\" \n",
    "    spacy has entity recognition in-built, which might work well\n",
    "    for names, but not for multi-word named entities (like movie titles)\n",
    "    \"\"\"\n",
    "    return doc.ents\n",
    "\n",
    "def custom_tokenizer(nlp):\n",
    "    \"\"\"\n",
    "    spacy gives the programmer the ability to customize the tokenizer using regex.\n",
    "    This one specifically looks for sets of contiguous words that all have an upper-\n",
    "    case letter (i.e. that are titled). This can alternatively be done by using spacy's\n",
    "    istitle() function on all combinations of words, but that is less efficient.\n",
    "    e.g. \"How I Met Your Mother\" will be a single token using this.\n",
    "    \"\"\"\n",
    "    token_re = re.compile(r\"([A-Z0-9]+[a-z']*(?:[\\s][A-Z][a-z]+|[\\s][0-9]+)*)\")\n",
    "    return Tokenizer(nlp.vocab, token_match = token_re.findall)\n",
    "\n",
    "def get_entity_complex(q_str):\n",
    "    \"\"\"\n",
    "    Calls the above function on a query string\n",
    "    \"\"\"\n",
    "    nlp.tokenizer = custom_tokenizer(nlp)\n",
    "    doc = nlp(q_str)\n",
    "    # return the last named entity since the needed \n",
    "    # entity is likely at the very end of the string\n",
    "    return doc[-1].text\n",
    "\n",
    "def get_closest_proper_noun(root, remove = ''):\n",
    "    \"\"\"\n",
    "    It is often the case that the proper noun\n",
    "    that is most closely associated with the root\n",
    "    is the most relevent entity in question.\n",
    "    This is a recursive function starting at the \n",
    "    root and doing a BFS through the tree\n",
    "    \"\"\"\n",
    "    pn = \"\"\n",
    "    for child in root.children:\n",
    "        if child.pos_ == 'PROPN':\n",
    "            pn = phrase(child, remove)\n",
    "            return pn\n",
    "        \n",
    "        pn = get_closest_proper_noun(child)\n",
    "        if pn != \"\":\n",
    "            break\n",
    "    \n",
    "    return pn\n",
    "\n",
    "def find_single_uppercase_entity(parse):\n",
    "    \"\"\"\n",
    "    If we are only searching for one entity,\n",
    "    we can just find the first and last index \n",
    "    of words beginning with an uppercase letter.\n",
    "    There is a check to make sure that the first \n",
    "    word isn't added if it isn't part of the entity.\n",
    "    \"\"\"\n",
    "    entities = []\n",
    "    entityRange = [] \n",
    "    \n",
    "    for i in range(len(parse)):\n",
    "        word = parse[i]\n",
    "        try:\n",
    "            secondWord = parse[i+1]\n",
    "        except:\n",
    "            secondWord = False\n",
    "        if word.text.istitle():\n",
    "            # Check if word starts with uppercase letter for entities\n",
    "            if i != 0 or (not is_exception(word.lemma_.lower())\n",
    "                          and (not secondWord or not is_exception(secondWord.lemma_))):\n",
    "                # If it isn't one of the question words (Who/What/Which/When) or a word before them\n",
    "                entityRange.append(i)\n",
    "    if len(entityRange) > 1:\n",
    "        minEntity = entityRange[0]\n",
    "        maxEntity = entityRange[len(entityRange)-1]\n",
    "        entity = parse[minEntity:maxEntity+1].text\n",
    "        entities.append(entity)\n",
    "    elif len(entityRange) == 1:\n",
    "        entity = parse[entityRange[0]].text\n",
    "        entities.append(entity)\n",
    "    return entities\n",
    "\n",
    "def get_entity_from_index_range(parse, begin, end):\n",
    "    return parse[begin:end+1].text\n",
    "\n",
    "def add_entity_based_on_index(numberRange, frontIndex, backIndex, parse):\n",
    "    entities = []\n",
    "    if len(numberRange) > 1:\n",
    "        minNumber = numberRange[0]\n",
    "        maxNumber = numberRange[len(numberRange)-1]\n",
    "        entities.append(get_entity_from_index_range(parse, minNumber, maxNumber))\n",
    "    for number in numberRange:\n",
    "        if not type(frontIndex) == bool:\n",
    "            # Add title parts from the front\n",
    "            entities.append(get_entity_from_index_range(parse, frontIndex, number))\n",
    "        if not type(backIndex) == bool:\n",
    "            # Add title parts from the back\n",
    "            if backIndex > number:\n",
    "                entities.append(get_entity_from_index_range(parse, number, backIndex))\n",
    "            else:\n",
    "                entities.append(get_entity_from_index_range(parse, backIndex, number))\n",
    "            if not type(frontIndex) == bool and not type(backIndex) == bool:\n",
    "                # Add Title parts from the front and back of this number\n",
    "                if backIndex > number:\n",
    "                    entities.append(get_entity_from_index_range(parse, frontIndex, backIndex))\n",
    "    return entities\n",
    "\n",
    "def find_single_number_entity(parse):\n",
    "    \"\"\"\n",
    "    Find an entity with a number in it.\n",
    "    Multiple possibilities will be returned \n",
    "    based on whether there are words starting \n",
    "    with uppercase letters before or after it.\n",
    "    \"\"\"\n",
    "    entities = []\n",
    "    numberRange = []\n",
    "    frontIndex = False \n",
    "    backIndex = False\n",
    "    \n",
    "    for i in range(len(parse)):\n",
    "        word = parse[i]\n",
    "        try:\n",
    "            secondWord = parse[i+1]\n",
    "        except:\n",
    "            secondWord = False\n",
    "        if word.pos_ == \"NUM\":\n",
    "            # Check if word is a number and add to range\n",
    "            numberRange.append(i)\n",
    "            # Add number itself as entity\n",
    "            entities.append(word.text)\n",
    "            # Show that number has been used\n",
    "            if type(backIndex) == bool:\n",
    "                backIndex = True\n",
    "        if word.text.istitle():\n",
    "            # Check if word starts with uppercase letter for entities\n",
    "            if i != 0 or (not is_exception(word.lemma_.lower())\n",
    "                          and (not secondWord or not is_exception(secondWord.lemma_))):\n",
    "                # If it isn't one of the question words (Who/What/Which/When) or a word before them\n",
    "                if not type(backIndex) == bool or backIndex == True:\n",
    "                    # number has already been used, so part after number\n",
    "                    backIndex = i\n",
    "                elif type(frontIndex) == bool:\n",
    "                    # First part of entity\n",
    "                    frontIndex = i\n",
    "    return entities + add_entity_based_on_index(numberRange, frontIndex, backIndex, parse)\n",
    "\n",
    "def entity_split_sentence(parse):\n",
    "    \"\"\"\n",
    "    Will return ranges for the sentences \n",
    "    so they contain at most one entity.\n",
    "    \"\"\"\n",
    "    ranges = []\n",
    "    minIndex = 0\n",
    "    maxIndex = False\n",
    "    for i in range(len(parse)):\n",
    "        word = parse[i]\n",
    "        try:\n",
    "            secondWord = parse[i+1]\n",
    "        except:\n",
    "            secondWord = False\n",
    "        if word.text.istitle():\n",
    "            # Check if word starts with uppercase letter for entities\n",
    "            if i != 0 or (not is_exception(word.lemma_.lower())\n",
    "                          and (not secondWord or not is_exception(secondWord.lemma_))):\n",
    "                    # If it isn't one of the question words (Who/What/Which/When) or a word before them\n",
    "                    if type(maxIndex) == bool:\n",
    "                        maxIndex = i\n",
    "                    elif i > maxIndex+2:\n",
    "                        ranges.append([minIndex, i-1])\n",
    "                        minIndex = i\n",
    "                        maxIndex = False\n",
    "                    else:\n",
    "                        maxIndex = i\n",
    "    ranges.append([minIndex, len(parse)-1])\n",
    "    return ranges\n",
    "    \n",
    "def find_uppercase_and_number_entities(parse):\n",
    "    entities = []\n",
    "    ranges = entity_split_sentence(parse)\n",
    "    for r in ranges:\n",
    "        subSentence = parse[r[0]:r[1]+1].text.replace('?','')\n",
    "        subSentence = nlp(subSentence)\n",
    "        uppercase = find_single_uppercase_entity(subSentence)\n",
    "        number = find_single_number_entity(subSentence)\n",
    "        entities += uppercase + number\n",
    "    return entities\n",
    "\n",
    "### Parser for natural language query ###\n",
    "\n",
    "def preprocess(query):\n",
    "    \"\"\"\n",
    "    Preprocessing for looking for entities using\n",
    "    non-dependency methods. This is not strictly\n",
    "    necessary, but makes it slightly less brittle\n",
    "    wrt the orthography of the sentence.\n",
    "    \"\"\"\n",
    "    query = query.replace('?','')\n",
    "    query = query.replace(query[0], query[0].lower(), 1)\n",
    "    return query\n",
    "\n",
    "def get_entities(doc):\n",
    "    \"\"\"\n",
    "    Return the possible entities of a given English query.\n",
    "    The possibilites are:\n",
    "        Proper noun phrase closest to root\n",
    "        Named entities according to SPACY\n",
    "        Regex-found entities (titled words in a row)\n",
    "    \"\"\"\n",
    "    entities = []\n",
    "    \n",
    "    root = get_root(doc)\n",
    "    entities.append(get_closest_proper_noun(root))\n",
    "    if q_type_binary(doc):\n",
    "        entities += find_uppercase_and_number_entities(doc)\n",
    "    else:\n",
    "        entities += find_single_uppercase_entity(doc)\n",
    "        entities += find_single_number_entity(doc)\n",
    "    query = preprocess(doc.text)\n",
    "    entities += [str(e) for e in get_named_entities(doc)]\n",
    "    entities.append(get_entity_complex(query))\n",
    "    \n",
    "    entities = [str(e) for e in entities if \" 's\" not in e]\n",
    "    \n",
    "    entities += [entity.replace(\"the \", \"\") for entity in entities]\n",
    "    entities += [entity.replace(\"'s\", \"\").strip() for entity in entities]\n",
    "    entities = remove_duplicates(entities)\n",
    "    \n",
    "    # Longer strings should be prioritized first\n",
    "    entities = sorted(entities, key=len, reverse=True)\n",
    "    \n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Property Extraction functions ###\n",
    "\n",
    "##Question types\n",
    "def q_type_addition(doc):\n",
    "    \"\"\"\n",
    "    Returns True iff the question word is prefixed by a preposition,\n",
    "    for example, if the question starts with In what, For how long, etc.\n",
    "    \"\"\"\n",
    "    return doc[0].dep == 'prep' or doc[len(doc)-1].dep == 'prep'\n",
    "\n",
    "def q_type_count(doc):\n",
    "    \"\"\"\n",
    "    Returns True iff the question asks for the number of results\n",
    "    \"\"\"\n",
    "    return check_key_words(doc) == 'number'\n",
    "\n",
    "def q_type_binary(doc):\n",
    "    \"\"\"\n",
    "    Returns True in and only in the case of a yes/no question\n",
    "    \"\"\"\n",
    "    return doc[0].lemma_ in ['be', 'do', 'have']\n",
    "\n",
    "def q_type_which_what_who(doc):\n",
    "    \"\"\"\n",
    "    Returns true if the first word or second word is which, what or who\n",
    "    (basic)\n",
    "    \"\"\"\n",
    "    wh = ['which','what','who']\n",
    "    return doc[0].text.lower() in wh or doc[1].text.lower() in wh\n",
    "\n",
    "def q_type_passive(doc):\n",
    "    \"\"\"\n",
    "    Returns true for a passive sentence\n",
    "    Normal passive sentences have a past particple of the verb\n",
    "    at the second word or the third word or the fourth word.\n",
    "    Would further need to check the sentence type of passive sentences.\n",
    "    \"\"\"\n",
    "    return doc[1].tag_ == 'VBN' or doc[2].tag_ == 'VBN' or doc[3].tag_ == 'VBN'\n",
    "\n",
    "def q_type_how_adj(doc):\n",
    "    \"\"\"\n",
    "    Returns true for how + adj sentences like\n",
    "    \"How long was Titanic?\"\n",
    "    \"\"\"\n",
    "    for token in doc:\n",
    "        if token.text.lower() == 'how':\n",
    "            return token.nbor().pos_ == 'ADJ' or token.nbor().pos_ == 'ADV' \n",
    "    return False          \n",
    "    \n",
    "def q_type_when(doc):\n",
    "    \"\"\"\n",
    "    Returns true if its a when questions eg \"When did Alan Rickman die?\"\n",
    "    Usually the when keyword is at the start.\n",
    "    \"\"\"\n",
    "    return doc[0].text.lower() == 'when' \n",
    "    \n",
    "def q_type_where(doc):\n",
    "    \"\"\"\n",
    "    Returns true if its a where question eg \"Where was Alan Rickman born?\"\n",
    "    Usually the where keyword is at the start.\n",
    "    \"\"\"\n",
    "    return doc[0].text.lower() == 'where'\n",
    "\n",
    "def get_root_related_props(doc, entities):\n",
    "    \"\"\"\n",
    "    Several methods to try and get properties with\n",
    "    respect to the root of the question.\n",
    "    \n",
    "    ps <- list of possible properties\n",
    "    For each child in root:\n",
    "        (i) it cannot be a property if it is an entity\n",
    "        (ii) it cannot be a property if it is a question word (w-word)\n",
    "        (iii) if it is a nominal subject, add it to ps\n",
    "        (iv) if it is a direct object, add it to ps\n",
    "        (v) if it is an adjective, add it to ps\n",
    "    If the root itself is not a simple word, add it to ps (e.g. if root := 'direct')\n",
    "    \n",
    "    return list of possible properties.\n",
    "    \n",
    "    Note: The lemmas and the phrases are added in order to make sure\n",
    "          multi-word properties (e.g. 'voice actor') are also considered\n",
    "    \"\"\"\n",
    "    ps = {}\n",
    "    root = get_root(doc)\n",
    "    \n",
    "    for child in root.children:\n",
    "        if len(entities) < 2 and phrase(child) in entities:\n",
    "            continue\n",
    "        if is_q_word(child.text.lower()):\n",
    "            continue\n",
    "        if child.dep_ == 'nsubj':\n",
    "            ps[phrase(child)] = 1\n",
    "            ps[child.text] = 1\n",
    "            ps[child.lemma_] = 1\n",
    "        if child.dep_ == 'dobj':\n",
    "            ps[phrase(child)] = 1\n",
    "            ps[child.text] = 1\n",
    "            ps[child.lemma_] = 1\n",
    "        if child.pos_ == 'ADJ':\n",
    "            ps[child.text] = 1\n",
    "            ps[child.lemma_] = 1\n",
    "    if root.lemma_ not in ['be', 'have', 'do']:\n",
    "        ps[root.text] = 1\n",
    "        ps[root.lemma_] = 1\n",
    "    \n",
    "    # Possibly add keyword\n",
    "    key_word = check_key_words(doc)\n",
    "    sorted_extended_ps = []\n",
    "    if key_word != None:\n",
    "        extended_ps = {}\n",
    "        extended_ps[key_word] = 3\n",
    "        for prop in list(ps.keys()):\n",
    "            extended_ps[prop + ' ' + key_word] = 2\n",
    "        sorted_extended_ps = list(dict(sorted(extended_ps.items(), key=lambda item: -item[1])).keys())\n",
    "    \n",
    "    # Sort in descending order of keys and convert to list\n",
    "    sorted_ps = list(dict(sorted(ps.items(), key=lambda item: -item[1])).keys())\n",
    "    \n",
    "    return sorted_ps, sorted_extended_ps\n",
    "\n",
    "def dumb_property_finder(parse) :\n",
    "    propRange = []\n",
    "    prop = \"\"\n",
    "    props = []\n",
    "    for i in range(len(parse)) : # iterate over the token objects \n",
    "        word = parse[i]\n",
    "    \n",
    "        if word.dep_ == \"ROOT\" and word.lemma_ not in ['be', 'have', 'do']:\n",
    "            # Set root as property when it isn't a form of to be, to have, or to do\n",
    "            prop = word.text\n",
    "    \n",
    "        if not word.text.istitle() and (word.pos_ == \"NOUN\" or word.pos_ == \"VERB\") and not is_exception(word.text):\n",
    "            # Properties are nouns or verbs\n",
    "            previousWord = parse[i-1]\n",
    "            if previousWord.pos_ == \"ADJ\" :\n",
    "                # Also add adjectives of the properties\n",
    "                propRange.append(i-1)\n",
    "                props.append(parse[i-1:i+1].text)\n",
    "            else:\n",
    "                props.append(parse[i:i+1].text)\n",
    "            propRange.append(i)\n",
    "    \n",
    "    if prop == \"\" and len(propRange) > 0:\n",
    "        minProp = propRange[0]\n",
    "        maxProp = propRange[len(propRange)-1]\n",
    "        prop = parse[minProp:maxProp+1].text\n",
    "    props.append(prop)\n",
    "    return props\n",
    "    \n",
    "def get_properties(doc, entity):\n",
    "    \"\"\"\n",
    "    Returns list of possible properties (list of strings)\n",
    "    \"\"\"\n",
    "    ps, extended_ps = get_root_related_props(doc, entity)\n",
    "    if q_type_binary(doc) or len(ps) == 0:\n",
    "        props = dumb_property_finder(doc)\n",
    "        if len(ps) == 0:\n",
    "            extended_ps += props\n",
    "        ps += props\n",
    "        for prop in ps:\n",
    "            ps += add_variations(prop)\n",
    "        ps = remove_duplicates(ps)\n",
    "    \n",
    "    # Remove all Nones\n",
    "    return [x for x in ps if x is not None], [x for x in extended_ps if x is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_query(entity_id, property_id, answer_id):\n",
    "    # This is the ask (YES/NO) query\n",
    "    query = f'''ASK {{\n",
    "       wd:{entity_id} wdt:{property_id} wd:{answer_id} .\n",
    "    }}'''\n",
    "    return query\n",
    "    \n",
    "def isNumber(string):\n",
    "    return type(string) == str and string.isdigit()\n",
    "    \n",
    "def orderAnswers(entities):\n",
    "    answers = []\n",
    "    # Add numbers as possible answers (for count ask queries)\n",
    "    for entity in entities:\n",
    "        if isNumber(entity):\n",
    "            answers.append(entity)\n",
    "    answers += get_wikidata_ids(entities)\n",
    "    return answers\n",
    "    \n",
    "def binary_queries(entity_id, property_id, answer_ids, non_ids):\n",
    "    non_ids.append(entity_id['id'])\n",
    "    non_ids.append(property_id['id'])\n",
    "    result = ''\n",
    "    answer = ''\n",
    "    for answer_id in answer_ids:\n",
    "        if isNumber(answer_id):\n",
    "            print(\"entity: \", entity_id['label'], entity_id['id'])\n",
    "            print(\"property: \", property_id['label'])\n",
    "            print(\"number: \", answer_id)\n",
    "            answer = answer_id\n",
    "            sparql_query = simple_sparql_query(entity_id['id'], property_id['id'])\n",
    "            result = get_SPARQL_results(sparql_query, True)\n",
    "            print(result)\n",
    "            if result == int(answer_id):\n",
    "                return 'Yes', answer\n",
    "        elif answer_id['id'] not in non_ids:\n",
    "            answer = answer_id['label']\n",
    "            sparql_query = binary_query(entity_id['id'], property_id['id'], answer_id['id'])\n",
    "            result = get_SPARQL_results(sparql_query)\n",
    "            if result == True:\n",
    "                return 'Yes', answer\n",
    "    return None, answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Does Pokémon have 1115', 'Does Pokémon', 'episodes', 'Pokémon', '1115']\n",
      "['Pokémon', '1115 episodes', 'episodes', 'episode', 'have', 'have 1115 episodes']\n",
      "[]\n",
      "['Pokémon', '1115 episodes', 'episodes', 'episode', 'have', 'have 1115 episodes']\n",
      "entity:  Pokémon Q97138261\n",
      "property:  Pokédex / Pokémon browser number\n",
      "number:  1115\n",
      "0\n",
      "entity:  Pokémon Q97138261\n",
      "property:  number of episodes\n",
      "number:  1115\n",
      "0\n",
      "entity:  Pokémon Q97138261\n",
      "property:  list of episodes\n",
      "number:  1115\n",
      "0\n",
      "entity:  Pokémon Q97138261\n",
      "property:  has part\n",
      "number:  1115\n",
      "26\n",
      "Closest answer:\n",
      "        entity: Pokémon\n",
      "      property: has part\n",
      "        answer: 11155 Kinpu\n",
      "\n",
      "answerLabel\tPokémon\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print(pipeline(\"Was Alan Rickman born in Hammersmith?\"))\n",
    "#print(pipeline('Is Denis Villeneuve the director of Blade Runner 2049?'))\n",
    "print(pipeline('Does Pokémon have 1115 episodes?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Functions that are majorly hueristic/custom ###\n",
    "from functools import lru_cache\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def get_synonyms(word, depth=1):\n",
    "    \"\"\"\n",
    "    Using WordNet (via NLTK), return synsets (synonyms/related words)\n",
    "    of a given word. Using the depth argument, the user can recursively \n",
    "    go down the tree of a given word's synonyms' synonyms to\n",
    "    get more words, but with probably less relevence, traversing\n",
    "    the tree in a BFS fasion. Most applications should just need \n",
    "    depth=1 (return just the first level of synonyms).\n",
    "    \"\"\"\n",
    "    # base case\n",
    "    if depth == 0:\n",
    "        return []\n",
    "    \n",
    "    # surface level synonyms\n",
    "    related_words = []\n",
    "    for syn in wn.synsets(word):\n",
    "        related_words += [x.name().replace('_', ' ') for x in syn.lemmas()]\n",
    "    \n",
    "    # deeper synonyms\n",
    "    for ls in [get_synonyms(x, depth-1) for x in related_words]:\n",
    "        related_words += ls\n",
    "    \n",
    "    # remove duplicates and return\n",
    "    return remove_duplicates(related_words)\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def get_movie_related_words(include_wordnet=True):\n",
    "    \"\"\"\n",
    "    Finds all (several) related words for entities in \n",
    "    the domain of movies. The top level have been hard-coded\n",
    "    and several more are found using WordNet's synsets.\n",
    "    This also means that not all returned words may be\n",
    "    strongly related to movies, just because of how WordNet\n",
    "    is designed.\n",
    "    \n",
    "    Note: Cached for speed using the lru_cache wrapper\n",
    "    \"\"\"\n",
    "    # naive relations, hand-written\n",
    "    # starting off point for synonym searching\n",
    "    movie_relation = ['movie', 'film', 'picture', 'moving picture', 'motion', 'pic', 'flick', 'TV',\n",
    "                      'television', 'show', 'animation', 'animation']\n",
    "    character_relation = ['fiction', 'fictitious', 'character']\n",
    "    actor_relation = ['actor', 'actress', 'thespian']\n",
    "    music_relation = ['musician', 'music', 'score', 'compose', 'song']\n",
    "    \n",
    "    all_relations = []\n",
    "    all_relations += movie_relation\n",
    "    all_relations += character_relation\n",
    "    all_relations += actor_relation\n",
    "    all_relations += music_relation\n",
    "    \n",
    "    if include_wordnet:\n",
    "        # get WordNet synsets\n",
    "        all_syns = [get_synonyms(x) for x in all_relations]\n",
    "\n",
    "        # add to relations\n",
    "        for syn in all_syns:\n",
    "            all_relations+=syn\n",
    "\n",
    "    # remove duplicates and return\n",
    "    return remove_duplicates(all_relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entity_related_to_movies(entity_list):\n",
    "    \"\"\"\n",
    "    Given a list of dictionaries with information about the entity,\n",
    "    check if the description contains a word that is related to a movie.\n",
    "    These have been chosen based on wordnet's synsets. This helps remove \n",
    "    non-relevent entities that have with the same name, but not related \n",
    "    to movies (e.g. Lord of the Rings book series).\n",
    "    \"\"\"\n",
    "    valid = []\n",
    "    all_relations = get_movie_related_words()\n",
    "    for word in all_relations:\n",
    "        for e in entity_list:\n",
    "            if 'description' in e.keys():\n",
    "                if word in e['description']:\n",
    "                    if e not in valid:\n",
    "                        valid.append(e)\n",
    "                    \n",
    "    return valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def permute(doc, entity_ids, property_ids, answer_ids, isCountQuestion = False):\n",
    "    # for each combination of entities and properties\n",
    "    # it is likely that the entities and properties\n",
    "    # are sorted by relevence/similarity by wikidata\n",
    "    # so return the first result that it finds. This\n",
    "    # is not guaranteed however\n",
    "    for entity_id in entity_ids:\n",
    "        for property_id in property_ids:\n",
    "            # print(entity_id['label'], property_id['label'])\n",
    "            if q_type_binary(doc):\n",
    "                non_ids = []\n",
    "                non = entity_related_to_movies(get_wikidata_ids(entity_id['label']))\n",
    "                for n in non:\n",
    "                    non_ids.append(n['id'])\n",
    "                result, answer_id = binary_queries(entity_id, property_id, answer_ids, non_ids)\n",
    "            else:\n",
    "                sparql_query = simple_sparql_query(entity_id['id'], property_id['id'])\n",
    "                result = get_SPARQL_results(sparql_query, isCountQuestion)\n",
    "\n",
    "            # if no result, try the reverse query\n",
    "            if result is None or result == '':\n",
    "                sparql_query = simple_sparql_query(entity_id['id'], property_id['id'], reverse = True)\n",
    "                result = get_SPARQL_results(sparql_query, isCountQuestion)\n",
    "\n",
    "            if result is not None and result != '':\n",
    "                print(\"Closest answer:\")\n",
    "                print(f\"        entity: {entity_id['label']}\")\n",
    "                print(f\"      property: {property_id['label']}\")\n",
    "                if q_type_binary(doc):\n",
    "                    print(f\"        answer: {answer_id}\")\n",
    "                print(\"\")\n",
    "                return result\n",
    "    return None\n",
    "\n",
    "def pipeline(question):\n",
    "    \"\"\"\n",
    "    Combines the above functions to create a pipeline to answer questions.\n",
    "    \n",
    "    Input: English question string\n",
    "    Output: Result (answer) of wikidata queries for that question\n",
    "    \"\"\"\n",
    "    result = ''\n",
    "    \n",
    "    # Load NLP model and tokenize/analize the question\n",
    "    nlp = spacy.load(\"en_core_web_trf\")\n",
    "    doc = nlp(question)\n",
    "    \n",
    "    # get entities & their ids\n",
    "    entities = get_entities(doc)\n",
    "    entity_ids = entity_related_to_movies(get_wikidata_ids(entities))\n",
    "    \n",
    "    # get properties\n",
    "    properties, extended_properties = get_properties(doc, entities)\n",
    "    \n",
    "    property_ids = get_wikidata_ids(properties, True)\n",
    "    extended_property_ids = get_wikidata_ids(extended_properties, True)\n",
    "    \n",
    "    answer_ids = []\n",
    "    if q_type_binary(doc):\n",
    "        answer_ids = orderAnswers(entities)\n",
    "        \n",
    "    print(entities)\n",
    "    print(properties)\n",
    "   \n",
    "    # wrap in a try/except to help with request errors\n",
    "    try:\n",
    "        result = permute(doc, entity_ids, extended_property_ids, answer_ids)\n",
    "        print(extended_properties)\n",
    "        if result == None:\n",
    "            print(properties)\n",
    "            result = permute(doc, entity_ids, property_ids, answer_ids, q_type_count(doc))\n",
    "        if result != None:\n",
    "            return result\n",
    "        if q_type_binary(doc):\n",
    "            return \"No\"\n",
    "            \n",
    "    except Exception:\n",
    "        print(\"Error while searching!\")\n",
    "    \n",
    "    # Guess\n",
    "    if q_type_binary(doc):\n",
    "        return \"Default binary, Yes\"\n",
    "    elif q_type_count(doc):\n",
    "        return \"Default count\"\n",
    "        #return \"1\"\n",
    "    return None\n",
    "\n",
    "def ask_question(question, base_answer=False):\n",
    "    # the main function used to ask queries\n",
    "    # it is mainly a wrapper for the pipeline\n",
    "    #try:\n",
    "    ans = pipeline(question)\n",
    "    #except Exception:\n",
    "    #    ans = \"Error\"\n",
    "    if ans is None:\n",
    "        print(\">>>>> Warning: Answer == None!\")\n",
    "        ans = \"Answer not found\"\n",
    "        \n",
    "    if base_answer: # strips the ans of any formatting\n",
    "        ans = ans.replace('answerLabel\\t', \"\").strip()\n",
    "        ans = ans.replace('\\n', \", \").strip()\n",
    "        \n",
    "    return ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question handling\n",
    "\n",
    "This QA system should be able to handle questions about movies of several types, but specifically desiged to be able to work with the following, with X being the property and Y being the entity:\n",
    "- Who/What/When/etc was/is/were the/a/an X of Y? (from previous assignment, more passive, noun properties)\n",
    "- Who/What/When/etc was/is/were Y X? (similar to above, more active, verb properties)\n",
    "- How X is Y? (similar questions that use adjective properties)\n",
    "\n",
    "The following are pairs of questions that the system is able to answer. These are in pairs to show that the same question that is phrased differently (as long as it follows an above format) should give the same answer. A noun property (e.g. height) can be translated to a adjective property (e.g. tall). Similarly, a verb property (acted) can be translated to a noun property (actor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: How many episodes does Twin Peaks have?\n",
      "['Twin Peaks', 'have']\n",
      "['How many episodes', 'episodes', 'episode', 'Twin Peaks', 'Peaks']\n",
      "['number', 'How many episodes number', 'episodes number', 'episode number', 'Twin Peaks number', 'Peaks number']\n",
      "['How many episodes', 'episodes', 'episode', 'Twin Peaks', 'Peaks']\n",
      "Closest answer:\n",
      "        entity: Twin Peaks: Fire Walk with Me\n",
      "      property: number of episodes\n",
      "\n",
      "0\n",
      "\t**********\n",
      "\n",
      "Query: How many awards did Titanic win?\n",
      "['Titanic', 'win']\n",
      "['How many awards', 'awards', 'award', 'Titanic', 'win']\n",
      "['number', 'How many awards number', 'awards number', 'award number', 'Titanic number', 'win number']\n",
      "['How many awards', 'awards', 'award', 'Titanic', 'win']\n",
      "Closest answer:\n",
      "        entity: Titanic\n",
      "      property: award received\n",
      "\n",
      "29\n",
      "\t**********\n",
      "\n",
      "Query: How many awards did George Clooney receive?\n",
      "['George Clooney', 'receive']\n",
      "['How many awards', 'awards', 'award', 'George Clooney', 'Clooney', 'receive']\n",
      "['number', 'How many awards number', 'awards number', 'award number', 'George Clooney number', 'Clooney number', 'receive number']\n",
      "['How many awards', 'awards', 'award', 'George Clooney', 'Clooney', 'receive']\n",
      "Closest answer:\n",
      "        entity: George Clooney\n",
      "      property: award received\n",
      "\n",
      "14\n",
      "\t**********\n",
      "\n",
      "Query: How many Pokémon episodes are there?\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Unable to load weights from pytorch checkpoint file for 'C:\\Users\\yara0\\anaconda3\\lib\\site-packages\\en_core_web_trf\\en_core_web_trf-3.0.0\\transformer\\model' at 'C:\\Users\\yara0\\anaconda3\\lib\\site-packages\\en_core_web_trf\\en_core_web_trf-3.0.0\\transformer\\model\\pytorch_model.bin'If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True. ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   1062\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1063\u001b[1;33m                 \u001b[0mstate_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresolved_archive_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"cpu\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1064\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    591\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 592\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    593\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_load\u001b[1;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[0;32m    850\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 851\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    852\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mpersistent_load\u001b[1;34m(saved_id)\u001b[0m\n\u001b[0;32m    842\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mloaded_storages\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 843\u001b[1;33m             \u001b[0mload_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_maybe_decode_ascii\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    844\u001b[0m         \u001b[0mstorage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloaded_storages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mload_tensor\u001b[1;34m(data_type, size, key, location)\u001b[0m\n\u001b[0;32m    830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 831\u001b[1;33m         \u001b[0mstorage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_storage_from_record\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    832\u001b[0m         \u001b[0mloaded_storages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrestore_location\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at ..\\c10\\core\\CPUAllocator.cpp:75] data. DefaultCPUAllocator: not enough memory: you tried to allocate 2359296 bytes. Buy new RAM!",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-107-67a5104163ab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mq\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcount_qs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Query: {q}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mask_question\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\t**********\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m#30; 30; 17; 1115; 40,000,000 United States dollar; 64; 5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-103-adb272b3d4d0>\u001b[0m in \u001b[0;36mask_question\u001b[1;34m(question, base_answer)\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[1;31m# it is mainly a wrapper for the pipeline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[1;31m#try:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m     \u001b[0mans\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m     \u001b[1;31m#except Exception:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m     \u001b[1;31m#    ans = \"Error\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-103-adb272b3d4d0>\u001b[0m in \u001b[0;36mpipeline\u001b[1;34m(question)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[1;31m# Load NLP model and tokenize/analize the question\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m     \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"en_core_web_trf\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m     \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\__init__.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \"\"\"\n\u001b[0;32m     50\u001b[0m     return util.load_model(\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdisable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m     )\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m    322\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mget_lang_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"blank:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_package\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# installed as package\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 324\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mload_model_from_package\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    325\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mPath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# path to model data directory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mload_model_from_package\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m    355\u001b[0m     \"\"\"\n\u001b[0;32m    356\u001b[0m     \u001b[0mcls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 357\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdisable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    358\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\en_core_web_trf\\__init__.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(**overrides)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mload_model_from_init_py\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mload_model_from_init_py\u001b[1;34m(init_file)\u001b[0m\n\u001b[0;32m    520\u001b[0m         \u001b[0mdisable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdisable\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    521\u001b[0m         \u001b[0mexclude\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 522\u001b[1;33m         \u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    523\u001b[0m     )\n\u001b[0;32m    524\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mload_model_from_path\u001b[1;34m(model_path)\u001b[0m\n\u001b[0;32m    390\u001b[0m     \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdict_to_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    391\u001b[0m     \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model_from_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdisable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 392\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_disk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    393\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    394\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36mfrom_disk\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m   1881\u001b[0m             \u001b[1;31m# Convert to list here in case exclude is (default) tuple\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1882\u001b[0m             \u001b[0mexclude\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexclude\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"vocab\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1883\u001b[1;33m         \u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_disk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeserializers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1884\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1885\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_link_components\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mfrom_disk\u001b[1;34m(path, readers, exclude)\u001b[0m\n\u001b[0;32m   1174\u001b[0m         \u001b[1;31m# Split to support file names like meta.json\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1175\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\".\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mexclude\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1176\u001b[1;33m             \u001b[0mreader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1177\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1178\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(p, proc)\u001b[0m\n\u001b[0;32m   1876\u001b[0m                 \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1877\u001b[0m             deserializers[name] = lambda p, proc=proc: proc.from_disk(\n\u001b[1;32m-> 1878\u001b[1;33m                 \u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"vocab\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1879\u001b[0m             )\n\u001b[0;32m   1880\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;34m\"vocab\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"vocab\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mexclude\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy_transformers\\pipeline_component.py\u001b[0m in \u001b[0;36mfrom_disk\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m    400\u001b[0m             \u001b[1;34m\"model\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    401\u001b[0m         }\n\u001b[1;32m--> 402\u001b[1;33m         \u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_disk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    403\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mfrom_disk\u001b[1;34m(path, readers, exclude)\u001b[0m\n\u001b[0;32m   1174\u001b[0m         \u001b[1;31m# Split to support file names like meta.json\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1175\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\".\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mexclude\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1176\u001b[1;33m             \u001b[0mreader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1177\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1178\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy_transformers\\pipeline_component.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(p)\u001b[0m\n\u001b[0;32m    390\u001b[0m             \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabsolute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    391\u001b[0m             tokenizer, transformer = huggingface_from_pretrained(\n\u001b[1;32m--> 392\u001b[1;33m                 \u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"tokenizer_config\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    393\u001b[0m             )\n\u001b[0;32m    394\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"tokenizer\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy_transformers\\util.py\u001b[0m in \u001b[0;36mhuggingface_from_pretrained\u001b[1;34m(source, config)\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mstr_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msource\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m     \u001b[0mtransformer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAutoModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m     \u001b[0mops\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_current_ops\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCupyOps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    358\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m             return cls._model_mapping[type(config)].from_pretrained(\n\u001b[1;32m--> 360\u001b[1;33m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    361\u001b[0m             )\n\u001b[0;32m    362\u001b[0m         raise ValueError(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   1064\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1065\u001b[0m                 raise OSError(\n\u001b[1;32m-> 1066\u001b[1;33m                     \u001b[1;34mf\"Unable to load weights from pytorch checkpoint file for '{pretrained_model_name_or_path}' \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1067\u001b[0m                     \u001b[1;34mf\"at '{resolved_archive_file}'\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1068\u001b[0m                     \u001b[1;34m\"If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True. \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: Unable to load weights from pytorch checkpoint file for 'C:\\Users\\yara0\\anaconda3\\lib\\site-packages\\en_core_web_trf\\en_core_web_trf-3.0.0\\transformer\\model' at 'C:\\Users\\yara0\\anaconda3\\lib\\site-packages\\en_core_web_trf\\en_core_web_trf-3.0.0\\transformer\\model\\pytorch_model.bin'If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True. "
     ]
    }
   ],
   "source": [
    "count_qs = ['How many episodes does Twin Peaks have?',\n",
    "     'How many awards did Titanic win?',\n",
    "      'How many awards did George Clooney receive?',\n",
    "      'How many Pokémon episodes are there?',\n",
    "      'What was the box office amount for the movie Psycho?',\n",
    "      'What is the total number of cast members of Iron Man?',\n",
    "      'What is the amount of Academy Award nominations that Morgan Freeman has?'\n",
    "     ]\n",
    "for q in count_qs:\n",
    "    print(f\"Query: {q}\")\n",
    "    print(ask_question(q))\n",
    "    print(\"\\t**********\\n\")\n",
    "#30; 30; 17; 1115; 40,000,000 United States dollar; 64; 5\n",
    "# Find 18 for number of episodes Twin Peaks, since two TV series with exact same name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: How many genres does Pulp Fiction have?\n",
      "Closest answer:\n",
      "        entity: Pulp Fiction\n",
      "      property: genre\n",
      "\n",
      "4\n",
      "\t**********\n",
      "\n",
      "Query: When was Alan Rickman born?\n",
      "Closest answer:\n",
      "        entity: Alan Rickman\n",
      "      property: date of birth\n",
      "\n",
      "answerLabel\t1946-02-21T00:00:00Z\n",
      "\n",
      "\t**********\n",
      "\n",
      "Query: Where was Alan Rickman born?\n",
      "Closest answer:\n",
      "        entity: Alan Rickman\n",
      "      property: place of birth\n",
      "\n",
      "answerLabel\tHammersmith\n",
      "\n",
      "\t**********\n",
      "\n",
      "Query: How many episodes does Twin Peaks have?\n",
      "Closest answer:\n",
      "        entity: Twin Peaks: Fire Walk with Me\n",
      "      property: number of episodes\n",
      "\n",
      "0\n",
      "\t**********\n",
      "\n",
      "Query: How long is Interstellar?\n",
      "Closest answer:\n",
      "        entity: Interstellar\n",
      "      property: duration\n",
      "\n",
      "answerLabel\t169\n",
      "\n",
      "\t**********\n",
      "\n",
      "Query: Who directed The Shawshank Redemption?\n",
      "Closest answer:\n",
      "        entity: The Shawshank Redemption\n",
      "      property: director\n",
      "\n",
      "answerLabel\tFrank Darabont\n",
      "\n",
      "\t**********\n",
      "\n",
      "Query: Who is the director of The Shawshank Redemption?\n",
      "Closest answer:\n",
      "        entity: The Shawshank Redemption\n",
      "      property: director\n",
      "\n",
      "answerLabel\tFrank Darabont\n",
      "\n",
      "\t**********\n",
      "\n",
      "Query: What is the birth date of Alan Rickman?\n",
      "Closest answer:\n",
      "        entity: Alan Rickman\n",
      "      property: date of birth\n",
      "\n",
      "answerLabel\t1946-02-21T00:00:00Z\n",
      "\n",
      "\t**********\n",
      "\n",
      "Query: When was Alan Rickman born?\n",
      "Closest answer:\n",
      "        entity: Alan Rickman\n",
      "      property: date of birth\n",
      "\n",
      "answerLabel\t1946-02-21T00:00:00Z\n",
      "\n",
      "\t**********\n",
      "\n",
      "Query: What is the height of Amitabh Bachchan?\n",
      ">>>>> Warning: Answer == None!\n",
      "Answer not found\n",
      "\t**********\n",
      "\n",
      "Query: How tall is Amitabh Bachchan?\n",
      ">>>>> Warning: Answer == None!\n",
      "Answer not found\n",
      "\t**********\n",
      "\n",
      "Query: What is the publication date of The Dark Knight?\n",
      "Closest answer:\n",
      "        entity: The Dark Knight\n",
      "      property: publication date\n",
      "\n",
      "answerLabel\t2008-07-18T00:00:00Z\n",
      "answerLabel\t2008-07-25T00:00:00Z\n",
      "answerLabel\t2008-08-13T00:00:00Z\n",
      "answerLabel\t2008-08-21T00:00:00Z\n",
      "\n",
      "\t**********\n",
      "\n",
      "Query: When was The Dark Knight published?\n",
      "Closest answer:\n",
      "        entity: The Dark Knight\n",
      "      property: publication date\n",
      "\n",
      "answerLabel\t2008-07-18T00:00:00Z\n",
      "answerLabel\t2008-07-25T00:00:00Z\n",
      "answerLabel\t2008-08-13T00:00:00Z\n",
      "answerLabel\t2008-08-21T00:00:00Z\n",
      "\n",
      "\t**********\n",
      "\n",
      "Query: Who acted as Gollum?\n",
      "Closest answer:\n",
      "        entity: Gollum\n",
      "      property: performer\n",
      "\n",
      "answerLabel\tAndy Serkis\n",
      "\n",
      "\t**********\n",
      "\n",
      "Query: Which actor played Gollum?\n",
      "Closest answer:\n",
      "        entity: Gollum\n",
      "      property: performer\n",
      "\n",
      "answerLabel\tAndy Serkis\n",
      "\n",
      "\t**********\n",
      "\n",
      "Query: What is the length of Interstellar?\n",
      "Closest answer:\n",
      "        entity: Interstellar\n",
      "      property: duration\n",
      "\n",
      "answerLabel\t169\n",
      "\n",
      "\t**********\n",
      "\n",
      "Query: How long does Interstellar run?\n",
      ">>>>> Warning: Answer == None!\n",
      "Answer not found\n",
      "\t**********\n",
      "\n",
      "Query: When did Alan Rickman die?\n",
      "Closest answer:\n",
      "        entity: Alan Rickman\n",
      "      property: date of birth\n",
      "\n",
      "answerLabel\t1946-02-21T00:00:00Z\n",
      "\n",
      "\t**********\n",
      "\n",
      "Query: When was Pulp Fiction published?\n",
      "Closest answer:\n",
      "        entity: Pulp Fiction\n",
      "      property: publication date\n",
      "\n",
      "answerLabel\t1994-05-21T00:00:00Z\n",
      "answerLabel\t1994-10-14T00:00:00Z\n",
      "answerLabel\t1994-11-03T00:00:00Z\n",
      "\n",
      "\t**********\n",
      "\n",
      "Query: Where was Morgan Freeman born?\n",
      "Closest answer:\n",
      "        entity: Morgan Freeman\n",
      "      property: place of birth\n",
      "\n",
      "answerLabel\tMemphis\n",
      "\n",
      "\t**********\n",
      "\n",
      "Query: Where does Home Alone originate?\n",
      "Closest answer:\n",
      "        entity: Home Alone\n",
      "      property: country of origin\n",
      "\n",
      "answerLabel\tUnited States of America\n",
      "\n",
      "\t**********\n",
      "\n",
      "Query: Which movies are directed by Alice Wu?\n",
      "Closest answer:\n",
      "        entity: Alice Wu\n",
      "      property: director\n",
      "\n",
      "answerLabel\tSaving Face\n",
      "answerLabel\tThe Half of It\n",
      "\n",
      "\t**********\n",
      "\n",
      "Query: How long is Pulp Fiction?\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-227-01c1d687b175>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mq\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlonely_qs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Query: {q}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mask_question\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\t**********\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;31m# Use \"en_core_web_trf\" instead of \"en_core_web_sm\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-226-3266ec2d643d>\u001b[0m in \u001b[0;36mask_question\u001b[1;34m(question, base_answer)\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[1;31m# it is mainly a wrapper for the pipeline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[1;31m#try:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m     \u001b[0mans\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m     \u001b[1;31m#except Exception:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[1;31m#    ans = \"Error\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-226-3266ec2d643d>\u001b[0m in \u001b[0;36mpipeline\u001b[1;34m(question)\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpermute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentity_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextended_property_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpermute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentity_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproperty_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mq_type_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-226-3266ec2d643d>\u001b[0m in \u001b[0;36mpermute\u001b[1;34m(doc, entity_ids, property_ids, isCountQuestion)\u001b[0m\n\u001b[0;32m     17\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m                 \u001b[0msparql_query\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msimple_sparql_query\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentity_id\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproperty_id\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreverse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_SPARQL_results\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msparql_query\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0misCountQuestion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-219-171a1baf3ff1>\u001b[0m in \u001b[0;36mget_SPARQL_results\u001b[1;34m(query, shouldBeCounted)\u001b[0m\n\u001b[0;32m    126\u001b[0m     \u001b[1;31m# Max 1000 attempts\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'query'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mquery\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'format'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'json'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Datasci\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'allow_redirects'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'get'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Datasci\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Datasci\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    540\u001b[0m         }\n\u001b[0;32m    541\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    543\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Datasci\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    653\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    654\u001b[0m         \u001b[1;31m# Send the request\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 655\u001b[1;33m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    656\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m         \u001b[1;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Datasci\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    437\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    438\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mchunked\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 439\u001b[1;33m                 resp = conn.urlopen(\n\u001b[0m\u001b[0;32m    440\u001b[0m                     \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m                     \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Datasci\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    697\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    698\u001b[0m             \u001b[1;31m# Make the request on the httplib connection object.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 699\u001b[1;33m             httplib_response = self._make_request(\n\u001b[0m\u001b[0;32m    700\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Datasci\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    380\u001b[0m         \u001b[1;31m# Trigger any extra validation we need to do.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 382\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_conn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    383\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m             \u001b[1;31m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Datasci\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1008\u001b[0m         \u001b[1;31m# Force connect early to allow us to validate the connection.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1009\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"sock\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# AppEngine might not have  `.sock`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1010\u001b[1;33m             \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1011\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1012\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_verified\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Datasci\\lib\\site-packages\\urllib3\\connection.py\u001b[0m in \u001b[0;36mconnect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    409\u001b[0m             \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_default_certs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    410\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 411\u001b[1;33m         self.sock = ssl_wrap_socket(\n\u001b[0m\u001b[0;32m    412\u001b[0m             \u001b[0msock\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    413\u001b[0m             \u001b[0mkeyfile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkey_file\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Datasci\\lib\\site-packages\\urllib3\\util\\ssl_.py\u001b[0m in \u001b[0;36mssl_wrap_socket\u001b[1;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[0;32m    400\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mca_certs\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mca_cert_dir\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mca_cert_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    401\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 402\u001b[1;33m             \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_verify_locations\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mca_certs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mca_cert_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mca_cert_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    403\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mIOError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mSSLError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "qs = ['How many genres does Pulp Fiction have?',\n",
    "     'When was Alan Rickman born?',\n",
    "     'Where was Alan Rickman born?',\n",
    "     'How many episodes does Twin Peaks have?',\n",
    "     'How long is Interstellar?']#,\n",
    "     #'Who is the director of Blade Runner 2049?']\n",
    "\n",
    "\n",
    "lonely_qs = ['Who directed The Shawshank Redemption?'\n",
    "     ,'Who is the director of The Shawshank Redemption?'\n",
    "      \n",
    "     ,'What is the birth date of Alan Rickman?'\n",
    "     ,'When was Alan Rickman born?'\n",
    "      \n",
    "     ,'What is the height of Amitabh Bachchan?'\n",
    "     ,'How tall is Amitabh Bachchan?'\n",
    "      \n",
    "     ,'What is the publication date of The Dark Knight?'\n",
    "     ,'When was The Dark Knight published?'\n",
    "     \n",
    "     ,'Who acted as Gollum?'\n",
    "     ,'Which actor played Gollum?',\n",
    "     \n",
    "     'What is the length of Interstellar?',\n",
    "     'How long does Interstellar run?',\n",
    "     'When did Alan Rickman die?',\n",
    "     'When was Pulp Fiction published?',\n",
    "     'Where was Morgan Freeman born?',\n",
    "     'Where does Home Alone originate?',\n",
    "     'Which movies are directed by Alice Wu?',\n",
    "     'How long is Pulp Fiction?',\n",
    "     'How many episodes does Twin Peaks have?',\n",
    "     'How long is Interstellar?',\n",
    "     'Which character was married to Aragorn?',\n",
    "     'Which character did Aragorn marry?']\n",
    "    \n",
    "for q in qs:\n",
    "    print(f\"Query: {q}\")\n",
    "    print(ask_question(q))\n",
    "    print(\"\\t**********\\n\")\n",
    "\n",
    "for q in lonely_qs:\n",
    "    print(f\"Query: {q}\")\n",
    "    print(ask_question(q))\n",
    "    print(\"\\t**********\\n\")\n",
    "# Use \"en_core_web_trf\" instead of \"en_core_web_sm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "['the height of Amitabh Bachchan', 'height']\n",
      "Closest answer:\n",
      "        entity: Amitabh Bachchan\n",
      "      property: height\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'answerLabel\\t1.88\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask_question(\"What is the height of Amitabh Bachchan?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from run_qs import get_q_list\n",
    "\n",
    "qs = [q[0] for q in get_q_list()[:3]]\n",
    "real_anss = [q[1] for q in get_q_list()[:3]]\n",
    "total_ans = 0\n",
    "not_found = 0\n",
    "\n",
    "f = open('answerlist.txt', 'w', encoding = 'utf-8')\n",
    "f.write('No|Query|Given Answer|System Answer\\n')\n",
    "\n",
    "for q, r_ans in zip(qs, real_anss):\n",
    "    \n",
    "    r_ans = \",\".join(r_ans)\n",
    "    \n",
    "    print(f'{total_ans}) Query: {q}')\n",
    "    \n",
    "    ans = ask_question(q, base_answer=True)\n",
    "    \n",
    "    try:\n",
    "        f.write(f'{total_ans}|{q}|{r_ans}|{ans}\\n')\n",
    "    except:\n",
    "        print(\">>>>> Could not write to file\")\n",
    "        print(f'>>>>> {total_ans}|{q}|{r_ans}|{ans}\\n')\n",
    "        \n",
    "    if ans in ['Answer not found', 'Error']:\n",
    "        not_found += 1\n",
    "        \n",
    "    total_ans += 1\n",
    "    \n",
    "    print (f'\\nAnswer: {ans}\\n')\n",
    "    print (f'Given Answer: {r_ans}\\n')\n",
    "\n",
    "f.close()\n",
    "\n",
    "print(f'Questions queried: {total_ans}')\n",
    "print(f'Not found ratio: {not_found/total_ans}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ask_question(\"How many episodes does Twin Peaks have?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('answerlist.txt', sep='|')\n",
    "\n",
    "print('Questions that could not be answered:')\n",
    "filtered = (df[df['System Answer']=='Answer not found'])\n",
    "\n",
    "for index, row in filtered.iterrows():\n",
    "    print(row[\"Query\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project\n",
    "\n",
    "For the final project, the goal is to implement a QA-sytem that will answer all kinds of questions about movies, actors, and everything related to the movie business.\n",
    "\n",
    "## Test Questions\n",
    "\n",
    "The result will be evaluated, among others, on a set of test questions. The test questions are provided as a tab-separated csv file consisting of an ID and the text of the question:\n",
    "\n",
    "    ID   Text of the question\n",
    "    \n",
    "Your results are also to be submitted as a tab-separated csv file. Code for reading in the data and writing the answers file is provided below. All you have to do is improve the question-answering function. \n",
    "\n",
    "## Answer file\n",
    "\n",
    "The answer file is also a tab-separated csv file, as in the example output below (in file 'our_team_answers.csv'). For list questions, return the list of answers separated by a comma, also as shown in the example below. \n",
    "\n",
    "Make sure to answer all questions, so if no answer is found by the system, insert a dummy answer such as 'No answer found'. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission by:\n",
    "### Team SREK üê∏\n",
    "Team members:\n",
    "- Joris Peters (s4001109)\n",
    "- Ruhi Mahadeshwar (s4014456)\n",
    "- Satchit Chatterji (s3889807)\n",
    "- Yara Bikowski (s3989585)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Program Code\n",
    "\n",
    "### (IO at the end of file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries\n",
    "NOTE: Please make sure these libraries are availible, and that the spacy model 'en_core_web_trf' is downloaded and loadable!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"NLP library\"\"\"\n",
    "import spacy\n",
    "\"\"\"Request operations\"\"\"\n",
    "import requests\n",
    "\"\"\"For regular expressions\"\"\"\n",
    "import re\n",
    "\"\"\"Tokenizer\"\"\"\n",
    "from spacy.tokenizer import Tokenizer\n",
    "\"\"\"Used for word similarity\"\"\"\n",
    "from nltk.corpus import wordnet as wn\n",
    "\"\"\"Cache similar function calls for speed\"\"\"\n",
    "from functools import lru_cache\n",
    "\"\"\"Regulate query times\"\"\"\n",
    "import datetime\n",
    "\n",
    "\"\"\"Load Spacy's large transformer model\"\"\"\n",
    "nlp = spacy.load('en_core_web_trf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_based_on_ids(id_list):\n",
    "    \"\"\"\n",
    "    If there are multiple ways of getting a list of properties,\n",
    "    then they may be repeated. This simply removes duplicates,\n",
    "    while not changing the relative order within the input list.\n",
    "    \"\"\"\n",
    "    id_set = {}\n",
    "    for obj in id_list:\n",
    "        id_set[obj['id']] = obj\n",
    "\n",
    "    return list(id_set.values())\n",
    "\n",
    "def get_wikidata_ids_of_word(name, search_property = False):\n",
    "    \"\"\"\n",
    "    Returns a list of ID dictionaries (with labels and possibly descriptions)\n",
    "    for a given name, either looking for entities or properties (set search_property:=True for the latter)\n",
    "    Each dict contains keys: 'id', 'label', and possibly 'description'.\n",
    "    If a description cannot be found, it will not be included in the dict.\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "    \n",
    "    url = 'https://www.wikidata.org/w/api.php'\n",
    "    params = {'action':'wbsearchentities', \n",
    "              'language':'en',\n",
    "              'format':'json'}\n",
    "    \n",
    "    # add a param to the request if it needs to look for a property\n",
    "    if search_property:\n",
    "        params['type'] = 'property'\n",
    "    \n",
    "    params['search'] = name\n",
    "    json = requests.get(url,params).json()\n",
    "    \n",
    "    # extract only the useful data from the json file\n",
    "    try:\n",
    "        for result in json['search']:\n",
    "            # append an empty dictionary\n",
    "            all_results.append({})\n",
    "            # add the ID and label\n",
    "            all_results[-1]['id'] = result['id']\n",
    "            all_results[-1]['label'] = result['label']\n",
    "            # add a description if it exists\n",
    "            if 'description' in result.keys():\n",
    "                all_results[-1]['description'] = result['description']\n",
    "    except Exception:\n",
    "        # no results\n",
    "        pass\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "def get_wikidata_ids(list_of_words, search_property = False):\n",
    "    \"\"\"\n",
    "    Returns a set of candidate id's for the list of words\n",
    "    \"\"\"\n",
    "    list_of_ids = []\n",
    "    for word in list_of_words:\n",
    "        list_of_ids += get_wikidata_ids_of_word(word, search_property)\n",
    "    # remove duplicates\n",
    "    set_of_ids = reduce_based_on_ids(list_of_ids)\n",
    "    return set_of_ids\n",
    "\n",
    "def simple_sparql_query(entity_id, property_id, binary_entity = None, reverse = False):\n",
    "    \"\"\" \n",
    "    Returns a SPARQL query string with a given entity and property, and, possibly,\n",
    "    reverses the arguments or adds an additional 'answer' for binary (Yes/No) questions\n",
    "    \"\"\"\n",
    "    if reverse:\n",
    "        p2 = \"wd:\" + entity_id\n",
    "        p1 = \"?answer\"\n",
    "    else:\n",
    "        p1 = \"wd:\" + entity_id\n",
    "        p2 = \"?answer\"\n",
    "        \n",
    "    if binary_entity != None:\n",
    "        query = f'''ASK {{\n",
    "            wd:{entity_id} wdt:{property_id} wd:{binary_entity} .\n",
    "        }}'''\n",
    "    else:\n",
    "        query = f'''SELECT ?answerLabel WHERE {{\n",
    "            {p1} wdt:{property_id} {p2}.\n",
    "            SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
    "        }}'''\n",
    "    return query\n",
    "    \n",
    "def property_qualifier_query(entity_id, property_id, qualifierEntity_id, qualifierProperty_id, reverse = False) :\n",
    "    \"\"\" \n",
    "    Returns a SPARQL query string for qualified sentence with two given entities and properties,\n",
    "    possibly reversing the arguments\n",
    "    \"\"\"\n",
    "    if reverse:\n",
    "        p2 = \"wd:\" + qualifierEntity_id\n",
    "        p1 = \"?item\"\n",
    "    else:\n",
    "        p1 = \"wd:\" + qualifierEntity_id\n",
    "        p2 = \"?item\"\n",
    "    query = f'''SELECT ?itemLabel WHERE {{ \n",
    "        wd:{entity_id} p:{property_id} ?stat . \n",
    "        ?stat ps:{property_id} {p1} . \n",
    "        ?stat pq:{qualifierProperty_id} {p2} .\n",
    "        SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
    "      }}'''\n",
    "    return query\n",
    "\n",
    "def simple_qualifier_query(entity_id, property_id):\n",
    "    \"\"\"\n",
    "    Use qualifier statement to also get a complete list of results.\n",
    "    \"\"\"\n",
    "    query = f'''SELECT ?itemLabel WHERE {{ \n",
    "        wd:{entity_id} p:{property_id} ?stat . \n",
    "        ?stat ps:{property_id} ?item .\n",
    "        SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
    "      }}'''\n",
    "    return query\n",
    "\n",
    "def get_SPARQL_results(query, shouldBeCounted = False):\n",
    "    \"\"\"\n",
    "    Return results (string) for a SPARQL query. In case it should be counted,\n",
    "    returns the number of results.\n",
    "    \"\"\"\n",
    "    url = 'https://query.wikidata.org/sparql'\n",
    "    if shouldBeCounted:\n",
    "        result = 0\n",
    "    else:\n",
    "        result = \"\"\n",
    "    # Max 1000 attempts\n",
    "    for _ in range(1000):\n",
    "        data = requests.get(url, params={'query': query, 'format': 'json'})\n",
    "        if data.status_code == 200:\n",
    "            break\n",
    "    data = data.json()\n",
    "    try:\n",
    "        return data['boolean']\n",
    "    except:\n",
    "        for item in data['results']['bindings']:\n",
    "            for var in item:\n",
    "                if shouldBeCounted:\n",
    "                    result += 1\n",
    "                else:\n",
    "                    result += ('{}\\t{}\\n'.format(var,item[var]['value']))\n",
    "    \n",
    "    if shouldBeCounted and result == 0:\n",
    "        return None\n",
    "    \n",
    "    return str(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linguistic helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_root(doc):\n",
    "    \"\"\"\n",
    "    Return the root of the dependency tree\n",
    "    in a given nlp-parsed sentence (root)\n",
    "    \"\"\"\n",
    "    for word in doc:\n",
    "        if word.dep_ == \"ROOT\":\n",
    "            return word\n",
    "        \n",
    "def is_q_word(string):\n",
    "    \"\"\"\n",
    "    Returns whether a string is a question word.\n",
    "    \"\"\"\n",
    "    return string in ['who', 'what', 'which', 'how', 'in', 'when', 'where', 'why', 'whom']\n",
    "                      \n",
    "def is_exception(string):\n",
    "    \"\"\"\n",
    "    Returns whether the word is a question word\n",
    "    or the begin word of a binary question.\n",
    "    \"\"\"\n",
    "    return string in ['was', 'is', 'does', 'did'] or (is_q_word(string) or string == \"in\")\n",
    "        \n",
    "def phrase(word, remove = \"\"):\n",
    "    \"\"\"\n",
    "    Given code: Return the phrase that the given word heads\n",
    "    \"\"\"\n",
    "    children = []\n",
    "    for child in word.subtree:\n",
    "        children.append(child.text.replace(remove,''))\n",
    "    return \" \".join(children)\n",
    "\n",
    "def empty_prop_id_dict(prop_id, label):\n",
    "    \"\"\"\n",
    "    Creates a dictionary with a given property id for qualifier shortcuts (see function below)\n",
    "    \"\"\"\n",
    "    return {\n",
    "        'id': prop_id,\n",
    "        'label': label,\n",
    "        'description': ''\n",
    "    }\n",
    "\n",
    "def check_qualified_words(doc):\n",
    "    \"\"\"\n",
    "    Given the presence of keywords in the document, this function returns a list of\n",
    "    id-dictionaries that will serve as shortcuts for answering qualified questions.\n",
    "    \"\"\"\n",
    "    key_words = {\n",
    "        'voice' : ['P453', 'P175', 'P725'],\n",
    "        'play' : ['P453', 'P161']\n",
    "    }\n",
    "    res = []\n",
    "    for token in doc:\n",
    "        one_word = token.lemma_\n",
    "        if token.pos_ == 'NUM':\n",
    "            res.append(empty_prop_id_dict(\"P585\", \"point in time\"))\n",
    "        if one_word in list(key_words.keys()):\n",
    "            res += [empty_prop_id_dict(x, one_word) for x in key_words[one_word]]\n",
    "    return res\n",
    "\n",
    "def check_key_words(doc):\n",
    "    \"\"\"\n",
    "    Check for special question words and other keywords. These will be appended to the properties.\n",
    "    \"\"\"\n",
    "    key_words = {\n",
    "        'how many' : 'number',\n",
    "        'quantity' : 'number',\n",
    "        'amount' : 'number',\n",
    "        'number of': 'number',\n",
    "        'how long' : 'duration',\n",
    "        'runtime' : 'duration',\n",
    "        'how often' : 'frequency',\n",
    "        'when' : 'date',\n",
    "        'where' : 'location',\n",
    "        'location' : 'location',\n",
    "        'why' : 'cause',\n",
    "        'cause' : 'cause',\n",
    "        'whose' : 'owner',\n",
    "        'birthday' : 'date of birth',\n",
    "        'directed' : 'director',\n",
    "        'about' : 'main subject',\n",
    "        'release':'publication'\n",
    "    }\n",
    "    for i in range(len(doc)-1):\n",
    "        one_word = doc[i].text.lower()\n",
    "        two_words = doc[i].text.lower() + \" \" + doc[i+1].text.lower()\n",
    "        if two_words in list(key_words.keys()):\n",
    "            return key_words[two_words]\n",
    "        elif one_word in list(key_words.keys()):\n",
    "            return key_words[one_word]\n",
    "\n",
    "        one_word = doc[i].lemma_.lower()\n",
    "        if one_word in list(key_words.keys()):\n",
    "            return key_words[one_word]\n",
    "\n",
    "    return None\n",
    "\n",
    "def add_variations(prop):\n",
    "    \"\"\"\n",
    "    A normalizer that adds all possible variations based on the given property.\n",
    "    \"\"\"\n",
    "    if prop == 'born' or prop == 'bear':\n",
    "        return ['birth place', 'birth date']\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "def remove_duplicates(ls):\n",
    "    \"\"\"\n",
    "    Removes duplicates from a list\n",
    "    \"\"\"\n",
    "    d = {}\n",
    "    for l in ls:\n",
    "        d[l] = 0\n",
    "    return list(d.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entity and Property extraction functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_named_entities(doc):\n",
    "    \"\"\" \n",
    "    spacy has entity recognition in-built, which might work well\n",
    "    for names, but not for multi-word named entities (like movie titles)\n",
    "    \"\"\"\n",
    "    return doc.ents\n",
    "\n",
    "def get_entity_complex(q_str):\n",
    "    \"\"\"\n",
    "    Using regex, the fuction finds the most likely entities. \n",
    "    Some example of entities it can find:\n",
    "    24, Fault In Our Stars, Inception, Blade Runner 2049,\n",
    "    12 Angry Men, Avengers: Endgame, WandaVision, Face/Off\n",
    "    \"\"\"\n",
    "    if is_exception(q_str.split()[0].lower()): \n",
    "        q_str = q_str.split(' ', 1)[1] #removing the first word since its mostly useless question word\n",
    "    entities = re.findall(r\"([A-Z0-9]+[a-z']*(?:[:/-]?[\\s]?[A-Z][a-z]+|[\\s][0-9]+)*)\", q_str)\n",
    "        \n",
    "    return entities\n",
    "\n",
    "\n",
    "def get_closest_proper_noun(root, remove = ''):\n",
    "    \"\"\"\n",
    "    It is often the case that the proper noun\n",
    "    that is most closely associated with the root\n",
    "    is the most relevent entity in question.\n",
    "    This is a recursive function starting at the \n",
    "    root and doing a BFS through the tree\n",
    "    \"\"\"\n",
    "    pn = \"\"\n",
    "    for child in root.children:\n",
    "        if child.pos_ == 'PROPN':\n",
    "            pn = phrase(child, remove)\n",
    "            return pn\n",
    "        \n",
    "        pn = get_closest_proper_noun(child)\n",
    "        if pn != \"\":\n",
    "            break\n",
    "    \n",
    "    return pn\n",
    "\n",
    "def find_single_uppercase_entity(parse):\n",
    "    \"\"\"\n",
    "    Used for binary questions:\n",
    "    If we are only searching for one entity,\n",
    "    we can just find the first and last index \n",
    "    of words beginning with an uppercase letter.\n",
    "    There is a check to make sure that the first \n",
    "    word isn't added if it isn't part of the entity.\n",
    "    \"\"\"\n",
    "    entities = []\n",
    "    entity_range = [] \n",
    "    \n",
    "    for i in range(len(parse)):\n",
    "        word = parse[i]\n",
    "        try:\n",
    "            second_word = parse[i+1]\n",
    "        except:\n",
    "            second_word = False\n",
    "        if word.text.istitle():\n",
    "            # Check if word starts with uppercase letter for entities\n",
    "            if i != 0 or (not is_exception(word.lemma_.lower())\n",
    "                          and (not second_word or not is_exception(second_word.lemma_))):\n",
    "                # If it isn't one of the question words (Who/What/Which/When) or a word before them\n",
    "                entity_range.append(i)\n",
    "    if len(entity_range) > 1:\n",
    "        min_entity = entity_range[0]\n",
    "        max_entity = entity_range[len(entity_range)-1]\n",
    "        entity = parse[min_entity:max_entity+1].text\n",
    "        entities.append(entity)\n",
    "    elif len(entity_range) == 1:\n",
    "        entity = parse[entity_range[0]].text\n",
    "        entities.append(entity)\n",
    "    return entities\n",
    "\n",
    "def get_entity_from_index_range(parse, begin, end):\n",
    "    \"\"\"\n",
    "    Return the text in the parsed text\n",
    "    based on the given begin and end index.\n",
    "    \"\"\"\n",
    "    return parse[begin:end+1].text\n",
    "\n",
    "def add_entity_based_on_index(number_range, front_index, back_index, parse):\n",
    "    \"\"\"\n",
    "    Based on the range, front index, and back index\n",
    "    return all possible variations of entities.\n",
    "    \"\"\"\n",
    "    entities = []\n",
    "    if len(number_range) > 1:\n",
    "        min_number = number_range[0]\n",
    "        max_number = number_range[len(number_range)-1]\n",
    "        entities.append(get_entity_from_index_range(parse, min_number, max_number))\n",
    "    for number in number_range:\n",
    "        if not type(front_index) == bool:\n",
    "            # Add title parts from the front\n",
    "            entities.append(get_entity_from_index_range(parse, front_index, number))\n",
    "        if not type(back_index) == bool:\n",
    "            # Add title parts from the back\n",
    "            if back_index > number:\n",
    "                entities.append(get_entity_from_index_range(parse, number, back_index))\n",
    "            else:\n",
    "                entities.append(get_entity_from_index_range(parse, back_index, number))\n",
    "            if not type(front_index) == bool and not type(back_index) == bool:\n",
    "                # Add Title parts from the front and back of this number\n",
    "                if back_index > number:\n",
    "                    entities.append(get_entity_from_index_range(parse, front_index, back_index))\n",
    "    return entities\n",
    "\n",
    "def find_single_number_entity(parse):\n",
    "    \"\"\"\n",
    "    Find an entity with a number in it.\n",
    "    Multiple possibilities will be returned \n",
    "    based on whether there are words starting \n",
    "    with uppercase letters before or after it.\n",
    "    \"\"\"\n",
    "    entities = []\n",
    "    number_range = []\n",
    "    front_index = False \n",
    "    back_index = False\n",
    "    \n",
    "    for i in range(len(parse)):\n",
    "        word = parse[i]\n",
    "        try:\n",
    "            second_word = parse[i+1]\n",
    "        except:\n",
    "            second_word = False\n",
    "        if word.pos_ == \"NUM\":\n",
    "            # Check if word is a number and add to range\n",
    "            number_range.append(i)\n",
    "            # Add number itself as entity\n",
    "            entities.append(word.text)\n",
    "            # Show that number has been used\n",
    "            if type(back_index) == bool:\n",
    "                back_index = True\n",
    "        if word.text.istitle():\n",
    "            # Check if word starts with uppercase letter for entities\n",
    "            if i != 0 or (not is_exception(word.lemma_.lower())\n",
    "                          and (not second_word or not is_exception(second_word.lemma_))):\n",
    "                # If it isn't one of the question words (Who/What/Which/When) or a word before them\n",
    "                if not type(back_index) == bool or back_index == True:\n",
    "                    # number has already been used, so part after number\n",
    "                    back_index = i\n",
    "                elif type(front_index) == bool:\n",
    "                    # First part of entity\n",
    "                    front_index = i\n",
    "    return entities + add_entity_based_on_index(number_range, front_index, back_index, parse)\n",
    "\n",
    "def entity_split_sentence(parse):\n",
    "    \"\"\"\n",
    "    Will return ranges for the sentences \n",
    "    so they contain at most one entity,\n",
    "    but some entities might be split.\n",
    "    \"\"\"\n",
    "    ranges = []\n",
    "    min_index = 0\n",
    "    max_index = False\n",
    "    for i in range(len(parse)):\n",
    "        word = parse[i]\n",
    "        try:\n",
    "            second_word = parse[i+1]\n",
    "        except:\n",
    "            second_word = False\n",
    "        if word.text.istitle():\n",
    "            # Check if word starts with uppercase letter for entities\n",
    "            if i != 0 or (not is_exception(word.lemma_.lower())\n",
    "                          and (not second_word or not is_exception(second_word.lemma_))):\n",
    "                    # If it isn't one of the question words (Who/What/Which/When) or a word before them\n",
    "                    if type(max_index) == bool:\n",
    "                        max_index = i\n",
    "                    elif i > max_index+2:\n",
    "                        ranges.append([min_index, i-1])\n",
    "                        min_index = i\n",
    "                        max_index = False\n",
    "                    else:\n",
    "                        max_index = i\n",
    "    ranges.append([min_index, len(parse)-1])\n",
    "    return ranges\n",
    "    \n",
    "def find_uppercase_and_number_entities(parse):\n",
    "    \"\"\"\n",
    "    First split the sentence if it contains multiple entities.\n",
    "    Then run both the uppercase and number entity finders \n",
    "    on them to find all entities in the sentence.\n",
    "    \"\"\"\n",
    "    entities = []\n",
    "    ranges = entity_split_sentence(parse)\n",
    "    for r in ranges:\n",
    "        sub_sentence = parse[r[0]:r[1]+1].text.replace('?','')\n",
    "        sub_sentence = nlp(sub_sentence)\n",
    "        uppercase = find_single_uppercase_entity(sub_sentence)\n",
    "        number = find_single_number_entity(sub_sentence)\n",
    "        entities += uppercase + number\n",
    "    return entities\n",
    "\n",
    "### Parser for natural language query ###\n",
    "\n",
    "def preprocess(query):\n",
    "    \"\"\"\n",
    "    Preprocessing for looking for entities using\n",
    "    non-dependency methods. This is not strictly\n",
    "    necessary, but makes it slightly less brittle\n",
    "    wrt the orthography of the sentence.\n",
    "    \"\"\"\n",
    "    query = query.replace('?','')\n",
    "    query = query.replace(query[0], query[0].lower(), 1)\n",
    "    return query\n",
    "\n",
    "def get_entities(doc):\n",
    "    \"\"\"\n",
    "    Return the possible entities of a given English query.\n",
    "    The possibilites are:\n",
    "        Proper noun phrase closest to root\n",
    "        Named entities according to SPACY\n",
    "        Regex-found entities (titled words in a row)\n",
    "    \"\"\"\n",
    "    entities = []\n",
    "    \n",
    "    root = get_root(doc)\n",
    "    entities.append(get_closest_proper_noun(root))\n",
    "    if q_type_binary(doc):\n",
    "        entities += find_uppercase_and_number_entities(doc)\n",
    "    else:\n",
    "        entities += find_single_uppercase_entity(doc)\n",
    "        entities += find_single_number_entity(doc)\n",
    "    query = preprocess(doc.text)\n",
    "    entities += [str(e) for e in get_named_entities(doc)]\n",
    "    entities.extend(get_entity_complex(query))\n",
    "    \n",
    "    entities = [str(e) for e in entities if \" 's\" not in e]\n",
    "    \n",
    "    entities += [entity.replace(\"the \", \"\") for entity in entities]\n",
    "    entities += [entity.replace(\"'s\", \"\").strip() for entity in entities]\n",
    "    entities = remove_duplicates(entities)\n",
    "    \n",
    "    # Longer strings should be prioritized first\n",
    "    entities = sorted(entities, key=len, reverse=True)\n",
    "    \n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Property Extraction functions ###\n",
    "\n",
    "##Question types\n",
    "def q_type_addition(doc):\n",
    "    \"\"\"\n",
    "    Returns True iff the question word is prefixed by a preposition,\n",
    "    for example, if the question starts with In what, For how long, etc.\n",
    "    \"\"\"\n",
    "    return doc[0].dep == 'prep' or doc[len(doc)-1].dep == 'prep'\n",
    "\n",
    "def q_type_count(doc):\n",
    "    \"\"\"\n",
    "    Returns True iff the question asks for the number of results\n",
    "    \"\"\"\n",
    "    return check_key_words(doc) == 'number'\n",
    "\n",
    "def q_type_date(doc):\n",
    "    \"\"\"\n",
    "    Returns True iff the question contains a number (presumably a date)\n",
    "    \"\"\"\n",
    "    return 'NUM' in [x.pos_ for x in doc]\n",
    "\n",
    "def q_type_qualifier(doc):\n",
    "    \"\"\"\n",
    "    Returns True iff the question might contain qualifiers\n",
    "    \"\"\"\n",
    "    return sum([x.dep_ in ['nsubj', 'dobj', 'pobj'] for x in doc]) > 2\n",
    "\n",
    "def q_type_easy_qualifier(doc):\n",
    "    \"\"\"\n",
    "    Returns True iff the question can be handled by some shortcuts\n",
    "    \"\"\"\n",
    "    return sum([x.lemma_ in ['in', 'character', 'play', 'voice'] for x in doc]) > 2\n",
    "\n",
    "def q_type_binary(doc):\n",
    "    \"\"\"\n",
    "    Returns True in and only in the case of a yes/no question\n",
    "    \"\"\"\n",
    "    return doc[0].lemma_ in ['be', 'do', 'have']\n",
    "\n",
    "def get_root_related_props(doc, entities):\n",
    "    \"\"\"\n",
    "    Several methods to try and get properties with\n",
    "    respect to the root of the question.\n",
    "    \n",
    "    Return list of possible properties.\n",
    "    \n",
    "    Note: The lemmas, text and/or phrases are added in order to make sure\n",
    "          multi-word properties (e.g. 'voice actor') are also considered\n",
    "    \"\"\"\n",
    "    ps = {}\n",
    "    root = get_root(doc)\n",
    "    \n",
    "    for child in root.children:\n",
    "        if len(entities) < 2 and phrase(child) in entities:\n",
    "            continue\n",
    "        if is_q_word(child.text.lower()):\n",
    "            continue\n",
    "        if child.dep_ == 'nsubj':\n",
    "            ps[phrase(child)] = 1\n",
    "            ps[child.text] = 1\n",
    "            ps[child.lemma_] = 1\n",
    "        if child.dep_ == 'dobj':\n",
    "            ps[phrase(child)] = 1\n",
    "            ps[child.text] = 1\n",
    "            ps[child.lemma_] = 1\n",
    "        if child.pos_ == 'ADJ':\n",
    "            ps[child.text] = 1\n",
    "            ps[child.lemma_] = 1\n",
    "    if root.lemma_ not in ['be', 'have', 'do']:\n",
    "        ps[root.lemma_] = 1\n",
    "        ps[root.text] = 1\n",
    "    for token in doc:\n",
    "        if token.dep_ == 'acl' and token.pos_ == 'VERB':\n",
    "            ps[token.lemma_] = 1\n",
    "    \n",
    "    # Possibly add keyword\n",
    "    key_word = check_key_words(doc)\n",
    "    sorted_extended_ps = []\n",
    "    if key_word != None:\n",
    "        extended_ps = {}\n",
    "        extended_ps[key_word] = 3\n",
    "        for prop in list(ps.keys()):\n",
    "            extended_ps[prop + ' ' + key_word] = 4\n",
    "        sorted_extended_ps = list(dict(sorted(extended_ps.items(), key=lambda item: -item[1])).keys())\n",
    "    \n",
    "    # Sort in descending order of keys and convert to list\n",
    "    sorted_ps = list(dict(sorted(ps.items(), key=lambda item: -item[1])).keys())\n",
    "    for token in doc:\n",
    "        if key_word == 'location' and 'die' == token.lemma_: \n",
    "            sorted_extended_ps = ['place of death']\n",
    "        if key_word == 'location' and 'bear' == token.lemma_: \n",
    "            sorted_extended_ps = ['place of birth']\n",
    "    return sorted_ps, sorted_extended_ps\n",
    "\n",
    "def dumb_property_finder(parse) :\n",
    "    \"\"\"\n",
    "    Find the property based on the root\n",
    "    or whether the word is a noun/verb\n",
    "    also add adjectives if present.\n",
    "    \"\"\"\n",
    "    begin_prop_index = False\n",
    "    end_prop_index = False\n",
    "    props = []\n",
    "    for i in range(len(parse)):\n",
    "        word = parse[i]\n",
    "    \n",
    "        if word.dep_ == \"ROOT\" and word.lemma_ not in ['be', 'have', 'do']:\n",
    "            # Set root as property when it isn't a form of to be, to have, or to do\n",
    "            props.append(word.lemma_)\n",
    "    \n",
    "        if (word.pos_ == \"NOUN\" or word.pos_ == \"VERB\") and not is_exception(word.text):\n",
    "            # Properties are nouns or verbs\n",
    "            props.append(phrase(word))\n",
    "            previous_word = parse[i-1]\n",
    "            if previous_word.pos_ == \"ADJ\" :\n",
    "                # Also add adjectives of the properties\n",
    "                if type(begin_prop_index) == bool:\n",
    "                    begin_prop_index = i-1\n",
    "                props.append(parse[i-1:i+1].lemma_)\n",
    "            if type(begin_prop_index) == bool:\n",
    "                begin_prop_index = i\n",
    "            end_prop_index = i\n",
    "            if q_type_count(parse) and word.text in ['amount', 'number']:\n",
    "                continue\n",
    "            props.append(parse[i].lemma_)\n",
    "        elif type(begin_prop_index) != bool and i > end_prop_index+1:\n",
    "            props.append(parse[begin_prop_index:end_prop_index+1].text)\n",
    "            begin_prop_index = False\n",
    "    \n",
    "    # Longer strings should be prioritized first\n",
    "    return props\n",
    "    \n",
    "def get_properties(doc, entity):\n",
    "    \"\"\"\n",
    "    Returns list of possible properties (list of strings)\n",
    "    \"\"\"\n",
    "    ps, extended_ps = get_root_related_props(doc, entity)\n",
    "    \n",
    "    if q_type_binary(doc) or not q_type_qualifier(doc) or q_type_count(doc):\n",
    "        props = dumb_property_finder(doc)\n",
    "        if q_type_binary(doc) or q_type_count(doc):\n",
    "            extended_ps += props\n",
    "            ps = props + ps\n",
    "        else:\n",
    "            ps += props\n",
    "    \n",
    "    if q_type_date(doc):\n",
    "        ps.append('point in time')\n",
    "    if q_type_qualifier(doc):\n",
    "        ps = [p for p in ps if p.lower() == p]\n",
    "    \n",
    "    # Remove all Nones\n",
    "    return [x for x in ps if x is not None], [x for x in extended_ps if x is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_number(string):\n",
    "    \"\"\"\n",
    "    Returns whether the string is actually a string\n",
    "    and whether this is a number.\n",
    "    \"\"\"\n",
    "    if type(string) == str:\n",
    "        string = string.replace(\".\",\"\")\n",
    "        string = string.replace(\",\",\"\")\n",
    "    else:\n",
    "        return False\n",
    "    return string.isdigit()\n",
    "    \n",
    "def order_answers(entities):\n",
    "    \"\"\"\n",
    "    From all entities, add the numbers separetly.\n",
    "    Then also add all entity ids to the answer ids.\n",
    "    \"\"\"\n",
    "    answers = []\n",
    "    # Add numbers as possible answers (for count ask queries)\n",
    "    for entity in entities:\n",
    "        if is_number(entity):\n",
    "            answers.append(entity)\n",
    "    answers += get_wikidata_ids(entities)\n",
    "    return answers\n",
    "    \n",
    "def binary_queries(entity_id, property_id, answer_ids, non_ids):\n",
    "    \"\"\"\n",
    "    Try to get an answer to the binary question.\n",
    "    If the answer is True, or the number \n",
    "    is equal to the answer number.\n",
    "    Otherwise return None.\n",
    "    \"\"\"\n",
    "    non_ids.append(entity_id['id'])\n",
    "    non_ids.append(property_id['id'])\n",
    "    result = ''\n",
    "    for answer_id in answer_ids:\n",
    "        if is_number(answer_id):\n",
    "            sparql_query = simple_sparql_query(entity_id['id'], property_id['id'])\n",
    "            result = get_SPARQL_results(sparql_query, True)\n",
    "            if result == int(answer_id):\n",
    "                return 'Yes'\n",
    "        elif answer_id['id'] not in non_ids:\n",
    "            sparql_query = simple_sparql_query(entity_id['id'], property_id['id'], answer_id['id'], False)\n",
    "            result = get_SPARQL_results(sparql_query)\n",
    "            if result == True:\n",
    "                return 'Yes'\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions that are majorly hueristic/custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_synonyms(word, depth=1):\n",
    "    \"\"\"\n",
    "    Using WordNet (via NLTK), return synsets (synonyms/related words)\n",
    "    of a given word. Using the depth argument, the user can recursively \n",
    "    go down the tree of a given word's synonyms' synonyms to\n",
    "    get more words, but with probably less relevence, traversing\n",
    "    the tree in a BFS fasion. Most applications should just need \n",
    "    depth=1 (return just the first level of synonyms).\n",
    "    \"\"\"\n",
    "    # base case\n",
    "    if depth == 0:\n",
    "        return []\n",
    "    \n",
    "    # surface level synonyms\n",
    "    related_words = []\n",
    "    for syn in wn.synsets(word):\n",
    "        related_words += [x.name().replace('_', ' ') for x in syn.lemmas()]\n",
    "    \n",
    "    # deeper synonyms\n",
    "    for ls in [get_synonyms(x, depth-1) for x in related_words]:\n",
    "        related_words += ls\n",
    "    \n",
    "    # remove duplicates and return\n",
    "    return remove_duplicates(related_words)\n",
    "\n",
    "@lru_cache(maxsize=1)\n",
    "def get_movie_related_words(include_wordnet=True):\n",
    "    \"\"\"\n",
    "    Finds all (several) related words for entities in \n",
    "    the domain of movies. The top level have been hard-coded\n",
    "    and several more are found using WordNet's synsets.\n",
    "    This also means that not all returned words may be\n",
    "    strongly related to movies, just because of how WordNet\n",
    "    is designed.\n",
    "    \n",
    "    Note: Cached for speed using the lru_cache wrapper\n",
    "    \"\"\"\n",
    "    # naive relations, hand-written\n",
    "    # starting off point for synonym searching\n",
    "    movie_relation = ['movie', 'film', 'picture', 'moving picture', 'motion', 'pic', 'flick', 'TV',\n",
    "                      'television', 'show', 'animation', 'animation']\n",
    "    character_relation = ['fiction', 'fictitious', 'character']\n",
    "    actor_relation = ['actor', 'actress', 'thespian']\n",
    "    music_relation = ['musician', 'music', 'score', 'compose', 'song']\n",
    "    \n",
    "    all_relations = []\n",
    "    all_relations += movie_relation\n",
    "    all_relations += character_relation\n",
    "    all_relations += actor_relation\n",
    "    all_relations += music_relation\n",
    "    \n",
    "    if include_wordnet:\n",
    "        # get WordNet synsets\n",
    "        all_syns = [get_synonyms(x) for x in all_relations]\n",
    "\n",
    "        # add to relations\n",
    "        for syn in all_syns:\n",
    "            all_relations+=syn\n",
    "\n",
    "    # remove duplicates and return\n",
    "    return remove_duplicates(all_relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entity_related_to_movies(entity_list):\n",
    "    \"\"\"\n",
    "    Given a list of dictionaries with information about the entity,\n",
    "    check if the description contains a word that is related to a movie.\n",
    "    These have been chosen based on wordnet's synsets. This helps remove \n",
    "    non-relevent entities that have with the same name, but not related \n",
    "    to movies (e.g. Lord of the Rings book series).\n",
    "    \"\"\"\n",
    "    valid = []\n",
    "    all_relations = get_movie_related_words()\n",
    "    for word in all_relations:\n",
    "        for e in entity_list:\n",
    "            if 'description' in e.keys():\n",
    "                if word in e['description']:\n",
    "                    if e not in valid:\n",
    "                        valid.append(e)\n",
    "                    \n",
    "    return valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def permute(doc, entity_ids, property_ids, answer_ids, is_qual_question = False, isCountQuestion = False):\n",
    "    \"\"\"\n",
    "    For each combination of entities and properties\n",
    "    it is likely that the entities and properties\n",
    "    are sorted by relevence/similarity by wikidata\n",
    "    so return the first result that it finds. This\n",
    "    is not guaranteed however. Furthermore, parameters\n",
    "    of whether the q should be handled with a qualified\n",
    "    query and if the result should be counted are handled.\n",
    "    \"\"\"\n",
    "    cur_time = datetime.datetime.now()\n",
    "    result = None\n",
    "    for entity_id in entity_ids:\n",
    "        for property_id in property_ids:\n",
    "            if datetime.datetime.now() - cur_time > datetime.timedelta(seconds = 300) and is_qual_question:\n",
    "                return None\n",
    "            if q_type_binary(doc):\n",
    "                non_ids = []\n",
    "                non = entity_related_to_movies(get_wikidata_ids(entity_id['label']))\n",
    "                for n in non:\n",
    "                    non_ids.append(n['id'])\n",
    "                result = binary_queries(entity_id, property_id, answer_ids, non_ids)\n",
    "            elif isCountQuestion:\n",
    "                sparql_query = simple_sparql_query(entity_id['id'], property_id['id'])\n",
    "                result = get_SPARQL_results(sparql_query, isCountQuestion)\n",
    "                if result not in [None, '']:\n",
    "                    temp_res = result\n",
    "                    sparql_query = simple_qualifier_query(entity_id['id'], property_id['id'])\n",
    "                    result = get_SPARQL_results(sparql_query, isCountQuestion)\n",
    "                    if result not in [None, '']:\n",
    "                        return result\n",
    "                    if result not in [None, '']:\n",
    "                        return result\n",
    "            elif is_qual_question:\n",
    "                for entity_id_2 in entity_ids:\n",
    "                    for property_id_2 in property_ids:\n",
    "                        if entity_id_2 != entity_id:\n",
    "                            for reverse in [True, False]:\n",
    "                                sparql_query = property_qualifier_query(entity_id['id'], property_id['id'],\n",
    "                                        entity_id_2['id'], property_id_2['id'], reverse)\n",
    "                                result = get_SPARQL_results(sparql_query)\n",
    "                                if result not in [None, '']:\n",
    "                                    return result\n",
    "            else:\n",
    "                for reverse in [True, False]:\n",
    "                    sparql_query = simple_sparql_query(entity_id['id'], property_id['id'], None, reverse)\n",
    "                    result = get_SPARQL_results(sparql_query, False)\n",
    "                    if q_type_count(doc):\n",
    "                        if not is_number(result.replace('answerLabel\\t', \"\").strip()):\n",
    "                            result = None\n",
    "                    if result not in [None, '']:\n",
    "                        return result\n",
    "\n",
    "    return result\n",
    "\n",
    "def pipeline(question):\n",
    "    \"\"\"\n",
    "    Combines the above functions to create a pipeline to answer questions.\n",
    "    \n",
    "    Input: English question string\n",
    "    Output: Result (answer) of wikidata queries for that question\n",
    "    \"\"\"\n",
    "    result = ''\n",
    "    \n",
    "    # Load NLP model and tokenize/analize the question\n",
    "    nlp = spacy.load(\"en_core_web_trf\")\n",
    "    doc = nlp(question)\n",
    "    \n",
    "    # get entities & their ids\n",
    "    entities = get_entities(doc)\n",
    "    entity_ids = entity_related_to_movies(get_wikidata_ids(entities))\n",
    "    \n",
    "    # get properties\n",
    "    properties, extended_properties = get_properties(doc, entities)\n",
    "    \n",
    "    # specific modifiers for yes/no questions\n",
    "    answer_ids = []\n",
    "    if q_type_binary(doc):\n",
    "        for propertyy in properties:\n",
    "            properties += add_variations(propertyy)\n",
    "        answer_ids = order_answers(entities)\n",
    "    \n",
    "    # get property ids\n",
    "    property_ids = get_wikidata_ids(properties, True)\n",
    "    property_ids = reduce_based_on_ids(property_ids)\n",
    "    extended_property_ids = get_wikidata_ids(extended_properties, True)\n",
    "    \n",
    "    # approach each type of question differently\n",
    "    if q_type_qualifier(doc):\n",
    "        qualifier_shortcuts = check_qualified_words(doc)\n",
    "        if qualifier_shortcuts:\n",
    "            # Qualifier shortcuts\n",
    "            result = permute(doc, entity_ids, qualifier_shortcuts, answer_ids, True)\n",
    "        if result in [None, '']:\n",
    "            # Qualified normal properties\n",
    "            result = permute(doc, entity_ids, property_ids, answer_ids, True)\n",
    "    if result in [None, '']:\n",
    "        # Keyword extended properties\n",
    "        result = permute(doc, entity_ids, extended_property_ids, answer_ids)\n",
    "        if result in [None, '']:\n",
    "            # Normal properties, possibly count\n",
    "            result = permute(doc, entity_ids, property_ids, answer_ids, False, q_type_count(doc))\n",
    "    \n",
    "    if result not in [None, '']:\n",
    "        return result\n",
    "\n",
    "    # Guesses if answer not found\n",
    "    if q_type_binary(doc):\n",
    "        return \"No\"\n",
    "    elif q_type_count(doc):\n",
    "        return '0'\n",
    "    \n",
    "    return None\n",
    "\n",
    "def ask_question(question, base_answer=True):\n",
    "    \"\"\" The main function used to ask queries\n",
    "       it is mainly a wrapper for the pipeline\"\"\"\n",
    "    ans = 'Answer not found'\n",
    "    try:\n",
    "        ans = pipeline(question)\n",
    "    except Exception:\n",
    "       ans = \"Error encoutered while searching!\"\n",
    "    \n",
    "    if ans in [None, '']:\n",
    "        ans = \"Answer not found\"\n",
    "        \n",
    "    if base_answer: # strips the ans of any formatting\n",
    "        ans = ans.replace('answerLabel\\t', \"\").strip()\n",
    "        ans = ans.replace('\\n', \", \").strip()\n",
    "        \n",
    "    return ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Input-Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = {}\n",
    "\n",
    "with open('test_questions.csv') as questions :\n",
    "    reader = csv.reader(questions,delimiter='\\t')\n",
    "    for row in reader:\n",
    "        question_id = row[0]\n",
    "        question_text = row[1]\n",
    "        print(f'Answering question: {question_id}')\n",
    "        answers[question_id] = ask_question(question_text)\n",
    "        \n",
    "with open('our_team_answers.csv', mode='w') as answerfile :\n",
    "    writer = csv.writer(answerfile,delimiter='\\t')\n",
    "    for key in answers :\n",
    "        writer.writerow([key,answers[key]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = input(\"Input your question!\")\n",
    "print(ask_question(q))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

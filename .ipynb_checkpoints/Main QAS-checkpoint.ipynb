{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Question Analysis\n",
    "\n",
    "The goal of this assignment is to write a more flexible version of the interactive QA system. As in the previous assignment, the system should be able to take a question in natural language (English) as input, analyse the question, and generate a SPARQL query for it.\n",
    "\n",
    "## Assignment  // Additional requirements\n",
    "\n",
    "* Make sure that your system can analyse at least two more question types. E.g. questions that start with *which*, *when*, where the property is expressed by a verb, etc.\n",
    "* Apart from the techniques introduced last week (matching tokens on the basis of their lemma or part-of-speech), also include at least one pattern where you use the dependency relations to find the relevant property or entity in the question. \n",
    "* Include 10 examples of questions that your system can handle, and that illustrate the fact that you cover additional question types\n",
    "\n",
    "## Examples\n",
    "\n",
    "Here is a non-representative list of questios and question types to consider. See the list with all questions for more examples\n",
    "\n",
    "* For what movie did Leonardo DiCaprio win an Oscar?\n",
    "* How long is Pulp Fiction?\n",
    "* How many episodes does Twin Peaks have?\n",
    "* In what capital was the film The Fault in Our Stars, filmed?\n",
    "* In what year was The Matrix released?\n",
    "* When did Alan Rickman die?\n",
    "* Where was Morgan Freeman born?\n",
    "* Which actor played Aragorn in Lord of the Rings?\n",
    "* Which actors played the role of James Bond\n",
    "* Who directed The Shawshank Redemption?\n",
    "* Which movies are directed by Alice Wu?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_trf\") # this loads the model for analysing English text\n",
    "                   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependency Analysis with Spacy\n",
    "\n",
    "All the functionality of Spacy, as in the last assignment, is still available for doing question analysis. \n",
    "\n",
    "In addition, also use the dependency relations assigned by spacy. Note that a dependency relation is a directed, labeled, arc between two tokens in the input. In the example below, the system detects that *movie* is the subject of the passive sentence (with label nsubjpass), and that the head of which this subject is a dependent is the word *are* with lemma *be*. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "which DET det movie\n",
      "movie NOUN nsubjpass direct\n",
      "be AUX auxpass direct\n",
      "direct VERB ROOT direct\n",
      "by ADP agent direct\n",
      "Alice PROPN compound Wu\n",
      "Wu PROPN pobj by\n",
      "? PUNCT punct direct\n"
     ]
    }
   ],
   "source": [
    "question = 'Which movies are directed by Alice Wu?'\n",
    "\n",
    "parse = nlp(question) # parse the input \n",
    "\n",
    "for word in parse : # iterate over the token objects \n",
    "    print(word.lemma_, word.pos_, word.dep_, word.head.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phrases\n",
    "\n",
    "You can also match with the full phrase that is the subject of the sentence, or any other dependency relation, using the subtree function \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which movies\n",
      "by Alice Wu\n"
     ]
    }
   ],
   "source": [
    "def phrase(word) :\n",
    "    children = []\n",
    "    for child in word.subtree :\n",
    "        children.append(child.text)\n",
    "    return \" \".join(children)\n",
    "        \n",
    "for word in parse:\n",
    "    if word.dep_ == 'nsubjpass' or word.dep_ == 'agent' :\n",
    "        phrase_text = phrase(word)\n",
    "        print(phrase_text)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation\n",
    "\n",
    "For a quick understanding of what the parser does, and how it assigns part-of-speech, entities, etc. you can also visualise parse results. Below, the entity visualiser and parsing visualiser is demonstrated. \n",
    "This code is for illustration only, it is not part of the assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Did \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Pamela Anderson\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " star in Borat?</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"32551663f3784f18a2ab1c2fe7c68351-0\" class=\"displacy\" width=\"1100\" height=\"312.0\" direction=\"ltr\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Did</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">Pamela</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">Anderson</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">star</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">in</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">Borat?</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-32551663f3784f18a2ab1c2fe7c68351-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,2.0 575.0,2.0 575.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-32551663f3784f18a2ab1c2fe7c68351-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-32551663f3784f18a2ab1c2fe7c68351-0-1\" stroke-width=\"2px\" d=\"M245,177.0 C245,89.5 395.0,89.5 395.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-32551663f3784f18a2ab1c2fe7c68351-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,179.0 L237,167.0 253,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-32551663f3784f18a2ab1c2fe7c68351-0-2\" stroke-width=\"2px\" d=\"M420,177.0 C420,89.5 570.0,89.5 570.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-32551663f3784f18a2ab1c2fe7c68351-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,179.0 L412,167.0 428,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-32551663f3784f18a2ab1c2fe7c68351-0-3\" stroke-width=\"2px\" d=\"M595,177.0 C595,89.5 745.0,89.5 745.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-32551663f3784f18a2ab1c2fe7c68351-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M745.0,179.0 L753.0,167.0 737.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-32551663f3784f18a2ab1c2fe7c68351-0-4\" stroke-width=\"2px\" d=\"M770,177.0 C770,89.5 920.0,89.5 920.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-32551663f3784f18a2ab1c2fe7c68351-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M920.0,179.0 L928.0,167.0 912.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "question = \"Did Pamela Anderson star in Borat?\"\n",
    "\n",
    "parse = nlp(question)\n",
    "\n",
    "displacy.render(parse, jupyter=True, style=\"ent\")\n",
    "\n",
    "displacy.render(parse, jupyter=True, style=\"dep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment Submission\n",
    "### S3889807"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code from last assignment\n",
    "- Get wikidata IDs\n",
    "- Generate SPARQL Queries\n",
    "- Connect to wikidata endpoint to get SPARQL results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def read_Qs():\n",
    "    lines = open('snippets\\\\all_qs.csv', 'r', encoding='utf8').readlines()\n",
    "    questions = []\n",
    "    for line in lines:\n",
    "        line=line.strip()\n",
    "        http_start_pos = line.find('http')\n",
    "        if http_start_pos == -1:\n",
    "            http_start_pos = line.find('wikidata')\n",
    "        if http_start_pos == -1:\n",
    "            http_start_pos = line.find('?')+1\n",
    "\n",
    "        q = line[:http_start_pos].strip()\n",
    "        words = line.split('\\t')\n",
    "        ans_pos = 0\n",
    "        for i in range(len(words)):\n",
    "            word = words[i]\n",
    "            if word.startswith('https'):\n",
    "                ans_pos = i+1\n",
    "                break\n",
    "        answers = words[ans_pos:]\n",
    "        questions.append((q, answers))\n",
    "    return questions\n",
    "\n",
    "def get_wikidata_ids(name, search_property = False):\n",
    "    \"\"\"\n",
    "    Returns a list of ID dictionaries (with labels and possibly descriptions)\n",
    "    for a given name, either looking for entities or properties (set search_property:=True for the latter)\n",
    "    Each dict contains keys: 'id', 'label', and possibly 'description'.\n",
    "    If a description cannot be found, it will not be included in the dict.\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "    \n",
    "    url = 'https://www.wikidata.org/w/api.php'\n",
    "    params = {'action':'wbsearchentities', \n",
    "              'language':'en',\n",
    "              'format':'json'}\n",
    "    \n",
    "    # add a param to the request if it needs to look for a property\n",
    "    if search_property:\n",
    "        params['type'] = 'property'\n",
    "    \n",
    "    params['search'] = name\n",
    "    json = requests.get(url,params).json()\n",
    "    \n",
    "    # extract only the useful data from the json file\n",
    "    try:\n",
    "        for result in json['search']:\n",
    "            # append an empty dictionary\n",
    "            all_results.append({})\n",
    "            # add the ID and label\n",
    "            all_results[-1]['id'] = result['id']\n",
    "            all_results[-1]['label'] = result['label']\n",
    "            # add a description if it exists\n",
    "            if 'description' in result.keys():\n",
    "                all_results[-1]['description'] = result['description']\n",
    "    except:\n",
    "        # no results\n",
    "        pass\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sparql_query(entity_id, property_id, extra):\n",
    "    \"\"\" \n",
    "    Returns string with entity id and property id in place as a SPARQL query\n",
    "    \"\"\"\n",
    "    query = f'''SELECT ?answerLabel WHERE {{\n",
    "                wd:{entity_id} wdt:{property_id} ?answer. {extra}\n",
    "                SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
    "                }}'''\n",
    "    return query\n",
    "\n",
    "def get_SPARQL_results(query):\n",
    "    \"\"\"\n",
    "    Relates to previous assignment. Return results (string) for a SPARQL query.\n",
    "    The format is arbitrary can can be changed as desired.\n",
    "    \"\"\"\n",
    "    url = 'https://query.wikidata.org/sparql'\n",
    "    results = \"\"\n",
    "    while True:\n",
    "        data = requests.get(url, params={'query': query, 'format': 'json'})\n",
    "        if data.status_code == 200:\n",
    "            break\n",
    "    data = data.json()\n",
    "    for item in data['results']['bindings']:\n",
    "        for var in item :\n",
    "            results+=('{}\\t{}\\n'.format(var,item[var]['value']))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Helpers\n",
    "\"\"\"\n",
    "\n",
    "def children(q, head, includeSelf):\n",
    "    \"\"\"\n",
    "    Returns direct children and self if needed\n",
    "    \"\"\"\n",
    "    children = []\n",
    "    for token in q:\n",
    "        if (token != head and token.head == head) or (includeSelf and token == head):\n",
    "            children.append(token)\n",
    "    return children\n",
    "\n",
    "def get_root(doc):\n",
    "    \"\"\"\n",
    "    Return the root of the dependency tree\n",
    "    in a given nlp-parsed sentence (root)\n",
    "    \"\"\"\n",
    "    for word in doc:\n",
    "        if word.dep_ == \"ROOT\":\n",
    "            return word\n",
    "        \n",
    "def phrase(word):\n",
    "    \"\"\"\n",
    "    Given code: Return the phrase that the given word heads\n",
    "    \"\"\"\n",
    "    children = []\n",
    "    for child in word.subtree :\n",
    "        children.append(child.text)\n",
    "    return \" \".join(children)\n",
    "\n",
    "def findDep(q, dep):\n",
    "    \"\"\"\n",
    "    Returns the first token corresponding to the dependency. Else false\n",
    "    \"\"\"\n",
    "    for word in q:\n",
    "        if word.dep_ in dep:\n",
    "            return word\n",
    "    return False\n",
    "\n",
    "def nominalize(word):\n",
    "    nom_dict = {\n",
    "        'much' : 'quantity',\n",
    "        'long' : 'duration',\n",
    "        'many' : 'quantity',\n",
    "        'often' : 'frequency'\n",
    "    }\n",
    "    return nom_dict[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Entity extraction functions ###\n",
    "\n",
    "import re\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "\n",
    "\n",
    "def get_named_entities(doc):\n",
    "    \"\"\" \n",
    "    spacy has entity recognition in-built, which might work well\n",
    "    for names, but not for multi-word named entities (like movie titles)\n",
    "    \"\"\"\n",
    "    return doc.ents\n",
    "\n",
    "def custom_tokenizer(nlp):\n",
    "    \"\"\"\n",
    "    spacy gives the programmer the ability to customize the tokenizer using regex.\n",
    "    This one specifically looks for sets of contiguous words that all have an upper-\n",
    "    case letter (i.e. that are titled). This can alternatively be done by using spacy's\n",
    "    istitle() function on all combinations of words, but that is less efficient.\n",
    "    e.g. \"How I Met Your Mother\" will be a single token using this.\n",
    "    \"\"\"\n",
    "    token_re = re.compile(r\"([A-Z][a-z']*(?:[\\s][A-Z][a-z]+)*)\")\n",
    "    return Tokenizer(nlp.vocab, token_match = token_re.findall)\n",
    "\n",
    "def get_entity_complex(q_str):\n",
    "    \"\"\"\n",
    "    Calls the above function on a query string\n",
    "    \"\"\"\n",
    "    nlp.tokenizer = custom_tokenizer(nlp)\n",
    "    doc = nlp(q_str)\n",
    "    # return the last named entity since the needed \n",
    "    # entity is likely at the very end of the string\n",
    "    return doc[-1].text\n",
    "\n",
    "def get_closest_proper_noun(root):\n",
    "    \"\"\"\n",
    "    It is often the case that the proper noun\n",
    "    that is most closely associated with the root\n",
    "    is the most relevent entity in question.\n",
    "    This is a recursive function starting at the \n",
    "    root and doing a BFS through the tree\n",
    "    \"\"\"\n",
    "    pn = None\n",
    "    for child in root.children:\n",
    "        if child.pos_ == 'PROPN':\n",
    "            pn = phrase(child)\n",
    "            return pn\n",
    "        \n",
    "        pn = get_closest_proper_noun(child)\n",
    "        if pn is not None:\n",
    "            break\n",
    "    \n",
    "    return pn\n",
    "\n",
    "### Parser for natural language query ###\n",
    "\n",
    "def preprocess(query):\n",
    "    \"\"\"\n",
    "    Preprocessing for looking for entities using\n",
    "    non-dependency methods. This is not strictly\n",
    "    necessary, but makes it slightly less brittle\n",
    "    wrt the orthography of the sentence.\n",
    "    \"\"\"\n",
    "    query = query.replace('?','')\n",
    "    query = query.replace(query[0], query[0].lower(), 1)\n",
    "    return query\n",
    "\n",
    "def get_entity(doc):\n",
    "    \"\"\"\n",
    "    Return the entity of a given English query.\n",
    "    The flow is:\n",
    "        Check if there is an entity according to\n",
    "        dependency tree\n",
    "        if yes:\n",
    "            return it\n",
    "        else:\n",
    "            preprocess query\n",
    "            x1 <- get named entities\n",
    "            x2 <- get entity with a custom tokenizer\n",
    "            return the longest string between x1 and x2\n",
    "    \"\"\"\n",
    "\n",
    "    entity, entity_temp = \"\", \"\"\n",
    "    \n",
    "    root = get_root(doc)\n",
    "    entity_temp = get_closest_proper_noun(root)\n",
    "    \n",
    "    if entity_temp is not None:\n",
    "        return entity_temp\n",
    "    \n",
    "    query = preprocess(query)\n",
    "    \n",
    "    entity_temp = get_named_entities(doc)\n",
    "    entity = entity_temp if len(entity_temp)>len(entity) else entity\n",
    "    \n",
    "    entity_temp = get_entity_complex(q)\n",
    "    entity = entity_temp if len(entity_temp)>len(entity) else entity\n",
    "    \n",
    "    return entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from queue import PriorityQueue\n",
    "\n",
    "### Property Extraction functions ###\n",
    "def reduce_based_on_ids(property_list):\n",
    "    \"\"\"\n",
    "    If there are multiple ways of getting a list of properties,\n",
    "    then they may be repeated. This simply removes duplicates,\n",
    "    while not changing the relative order within the input list.\n",
    "    \"\"\"\n",
    "    p_set = {}\n",
    "    for p in property_list:\n",
    "        p_set[p['id']] = p\n",
    "\n",
    "    return list(p_set.values())\n",
    "\n",
    "def q_type_addition(doc):\n",
    "    \"\"\"\n",
    "    Returns True iff the question word is prefixed by a preposition,\n",
    "    for example, if the question starts with In what, For how long, etc.\n",
    "    \"\"\"\n",
    "    if doc[0].dep == 'prep' or doc[len(doc)-1].dep == 'prep':\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def q_type_binary(doc):\n",
    "    \"\"\"\n",
    "    Returns True in and only in the case of a yes/no question\n",
    "    \"\"\"\n",
    "    return doc[0].lemma_ in ['be', 'do', 'have']\n",
    "\n",
    "def questionType(doc):\n",
    "    \"\"\"\n",
    "    Select question type\n",
    "    \"\"\"\n",
    "    options = {\n",
    "            'What' : whatOrWho,\n",
    "            'Who' : whatOrWho,\n",
    "            'When' : whenOrWhere,\n",
    "            'Where' : whenOrWhere,\n",
    "            'Howlong' : howLong,\n",
    "            'Howmany' : howMany}\n",
    "    if (q[0].text+q[1].text in options):\n",
    "        options[q[0].text+q[1].text](q)\n",
    "    elif (q[0].text in options):\n",
    "        options[q[0].text](q)\n",
    "    else:\n",
    "        print(\"question type not supported, but we'll try...\")\n",
    "\n",
    "def get_root_related_props(doc, entity):\n",
    "    \"\"\"\n",
    "    Several methods to try and get properties with\n",
    "    respect to the root of the question.\n",
    "    \n",
    "    ps <- list of possible properties\n",
    "    For each child in root:\n",
    "        (i) it cannot be a property if it is the entity\n",
    "        (ii) it cannot be a property if it is a question word (w-word)\n",
    "        (iii) if it is a nominal subject, add it to ps\n",
    "        (iv) if it is a direct object, add it to ps\n",
    "        (v) if it is an adjective, add it to ps\n",
    "    If the root itself is not a simple word, add it to ps (e.g. if root := 'direct')\n",
    "    \n",
    "    return list of possible properties.\n",
    "    \n",
    "    Note: The lemmas and the phrases are added in order to make sure\n",
    "          multi-word properties (e.g. 'voice actor') are also considered\n",
    "    \"\"\"\n",
    "    ps = []\n",
    "    root = get_root(doc)\n",
    "\n",
    "    for child in root.children:\n",
    "        if phrase(child) == entity:\n",
    "            continue\n",
    "        if child.text.lower() in ['who', 'what', 'when', 'how', 'which']:\n",
    "            continue\n",
    "        if child.dep_ == 'nsubj':\n",
    "            ps.append(phrase(child))\n",
    "            ps.append(child.text)\n",
    "        if child.dep_ == 'dobj':\n",
    "            ps.append(phrase(child))\n",
    "            ps.append(child.text)\n",
    "        if child.pos_ == 'ADJ':\n",
    "            ps.append(nominalize(child.lemma_))\n",
    "            ps.append(child.text)\n",
    "            ps.append(child.lemma_)\n",
    "    if root.lemma_ not in ['be', 'have', 'do']:\n",
    "        ps.append(root.text)\n",
    "        ps.append(root.lemma_)\n",
    "    return ps\n",
    "\n",
    "def get_root_related_props_binary(doc, entity):\n",
    "    ps = PriorityQueue()\n",
    "    entity_2 = []\n",
    "    root = get_root(doc)\n",
    "    \n",
    "    for child in root.children:\n",
    "        if phrase(child) == entity:\n",
    "            continue\n",
    "        if child.text.lower() in ['who', 'what', 'when', 'how', 'which']:\n",
    "            continue\n",
    "        if child == root and child.pos_ == 'VERB' and not in ['be', 'have', 'do']:\n",
    "            ps.put((10, root.text))\n",
    "            ps.put((10, root.lemma_))\n",
    "        if child.dep_ == 'nsubj':\n",
    "            ps.put((phrase(child)))\n",
    "            ps.append(child.text)\n",
    "        if child.dep_ == 'dobj':\n",
    "            ps.append((phrase(child)))\n",
    "            ps.append(child.text)\n",
    "        if child.pos_ == 'ADJ':\n",
    "            ps.append(nominalize(child.lemma_)))\n",
    "            ps.append(child.text))\n",
    "            ps.append(child.lemma_))\n",
    "            \n",
    "    final_ps = []\n",
    "    while not q.empty():\n",
    "        final_ps.append(q.get())\n",
    "    return ps\n",
    "\n",
    "def get_properties(doc, entity):\n",
    "    \"\"\"\n",
    "    Returns list of possible properties (list of strings)\n",
    "    \"\"\"\n",
    "    \n",
    "    ps = get_root_related_props(doc, entity)\n",
    "\n",
    "    # remove all Nones\n",
    "    return [x for x in ps if x is not None]\n",
    "\n",
    "def get_properties_binary(doc, entity):\n",
    "    \"\"\"\n",
    "    Returns list of possible properties (list of strings)\n",
    "    \"\"\"\n",
    "    \n",
    "    ps = get_root_related_props_binary(doc, entity)\n",
    "\n",
    "    # remove all Nones\n",
    "    return [x for x in ps if x is not None]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Functions that are majorly hueristic/custom ###\n",
    "\n",
    "def entity_related_to_movies(entity_list):\n",
    "    \"\"\"\n",
    "    Given a list of dictionaries with information about the entity,\n",
    "    check if the description contains a word that is related to a movie.\n",
    "    These have been chosen based on wordnet's synsets. This can easily be\n",
    "    extended or made more complex using nltk, but kept straightforward for now\n",
    "    as it works well enough. This helps remove non-relevent entities that have\n",
    "    with the same name, but not related to movies (e.g. Lord of the Rings book series)\n",
    "    \"\"\"\n",
    "    valid = []\n",
    "    movie_relation = ['movie', 'film', 'picture', 'moving picture', 'motion', 'pic', 'flick', 'TV',\n",
    "                      'television', 'show', 'animation', 'animation']\n",
    "    character_relation = ['fiction', 'fictitious', 'character']\n",
    "    actor_relation = ['actor', 'actress', 'thespian']\n",
    "    for word in movie_relation+character_relation+actor_relation:\n",
    "        for e in entity_list:\n",
    "            if 'description' in e.keys():\n",
    "                if word in e['description']:\n",
    "                    if e not in valid:\n",
    "                        valid.append(e)\n",
    "                    \n",
    "    return valid\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(query):\n",
    "    \"\"\"\n",
    "    Combines the above functions to create a pipeline to answer questions.\n",
    "    \n",
    "    Input: English string of the form \"Who/What was/is/were (the) X of Y?\"\n",
    "    Output: Result of that query if found\n",
    "\n",
    "    Often if there are too many queries sent to the endpoint at once,\n",
    "    it will return none, so an optional boolean moderation is added\n",
    "    to add an artificial 0.5 seconds between each request. This can be slower\n",
    "    but has a lower chance of producing a request-related error. If an error\n",
    "    occurs, try running with moderated = True\n",
    "    \"\"\"\n",
    "    result = ''\n",
    "    nlp = spacy.load(\"en_core_web_trf\") # this loads the model for analysing English text\n",
    "    doc = nlp(query)\n",
    "    \n",
    "    isBinary = q_type_binary(doc)\n",
    "    \n",
    "    # get entities\n",
    "    entity = get_entity(doc)\n",
    "    entity = entity.replace(\"the \", \"\")\n",
    "    entity_ids = entity_related_to_movies(get_wikidata_ids(entity))\n",
    "    \n",
    "    # get properties\n",
    "    if isBinary:\n",
    "        properties = get_properties_binary(doc, entity)\n",
    "    else:\n",
    "        properties = get_properties(doc, entity)\n",
    "    \n",
    "    property_ids = []\n",
    "    for p in properties:\n",
    "        property_ids += get_wikidata_ids(p, True)\n",
    "     \n",
    "    # remove duplicates\n",
    "    property_ids = reduce_based_on_ids(property_ids)\n",
    "\n",
    "    # for each combination of entities and properties\n",
    "    # it is likely that the entities and properties\n",
    "    # are sorted by relevence/similarity by wikidata\n",
    "    # so return the first result that it finds. This\n",
    "    # is not guaranteed however\n",
    "    for entity_id in entity_ids:\n",
    "        for property_id in property_ids:\n",
    "            print(entity_id['label'], property_id['label'])\n",
    "            # general SPARQL query\n",
    "            sparql_query = generate_sparql_query(entity_id['id'], property_id['id'], \"\")\n",
    "\n",
    "            result = get_SPARQL_results(sparql_query)\n",
    "\n",
    "            # Check for result\n",
    "            if result is not None and result != '':\n",
    "                print(f\"        entity: {entity_id['label']}\")\n",
    "                print(f\"      property: {property_id['label']}\\n\")\n",
    "                return result\n",
    "\n",
    "    return \"Answer not found\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question handling\n",
    "\n",
    "This QA system should be able to handle questions about movies of several types, but specifically desiged to be able to work with the following, with X being the property and Y being the entity:\n",
    "- Who/What/When/etc was/is/were the/a/an X of Y? (from previous assignment, more passive, noun properties)\n",
    "- Who/What/When/etc was/is/were Y X? (similar to above, more active, verb properties)\n",
    "- How X is Y? (similar questions that use adjective properties)\n",
    "\n",
    "The following are pairs of questions that the system is able to answer. These are in pairs to show that the same question that is phrased differently (as long as it follows an above format) should give the same answer. A noun property (e.g. height) can be translated to a adjective property (e.g. tall). Similarly, a verb property (acted) can be translated to a noun property (actor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: How long does Interstellar run?\n",
      "Interstellar duration\n",
      "        entity: Interstellar\n",
      "      property: duration\n",
      "\n",
      "answerLabel\t169\n",
      "\n",
      "\t**********\n",
      "\n",
      "Query: When was Pulp Fiction published?\n",
      "Pulp Fiction publication date\n",
      "        entity: Pulp Fiction\n",
      "      property: publication date\n",
      "\n",
      "answerLabel\t1994-05-21T00:00:00Z\n",
      "answerLabel\t1994-10-14T00:00:00Z\n",
      "answerLabel\t1994-11-03T00:00:00Z\n",
      "\n",
      "\t**********\n",
      "\n",
      "Query: Where was Morgan Freeman born?\n",
      "Morgan Freeman date of birth\n",
      "        entity: Morgan Freeman\n",
      "      property: date of birth\n",
      "\n",
      "answerLabel\t1937-06-01T00:00:00Z\n",
      "\n",
      "\t**********\n",
      "\n",
      "Query: Where does Home Alone originate?\n",
      "Home Alone country of origin\n",
      "        entity: Home Alone\n",
      "      property: country of origin\n",
      "\n",
      "answerLabel\tUnited States of America\n",
      "\n",
      "\t**********\n",
      "\n",
      "Query: Which movies are directed by Alice Wu?\n",
      "Alice Wu director\n",
      "Alice Wu editor-in-chief\n",
      "Alice Wu directions\n",
      "Alice Wu director of photography\n",
      "Alice Wu Curlie ID\n",
      "Alice Wu direction\n",
      "Alice Wu Directory of Open Access Journals ID\n",
      "Alice Wu organization directed by the office or position\n",
      "Answer not found\n",
      "\t**********\n",
      "\n",
      "Query: How long is Pulp Fiction?\n",
      "Pulp Fiction duration\n",
      "        entity: Pulp Fiction\n",
      "      property: duration\n",
      "\n",
      "answerLabel\t154\n",
      "\n",
      "\t**********\n",
      "\n",
      "Query: How many episodes does Twin Peaks have?\n",
      "Twin Peaks: Fire Walk with Me number of episodes\n",
      "Twin Peaks: Fire Walk with Me list of episodes\n",
      "Twin Peaks number of episodes\n",
      "        entity: Twin Peaks\n",
      "      property: number of episodes\n",
      "\n",
      "answerLabel\t18\n",
      "\n",
      "\t**********\n",
      "\n",
      "Query: How long is Interstellar?\n",
      "Interstellar duration\n",
      "        entity: Interstellar\n",
      "      property: duration\n",
      "\n",
      "answerLabel\t169\n",
      "\n",
      "\t**********\n",
      "\n",
      "Query: Which character was married by Aragorn\n",
      "Aragorn spouse\n",
      "Aragorn place of marriage\n",
      "Aragorn married name\n",
      "Aragorn spouse\n",
      "        entity: Aragorn\n",
      "      property: spouse\n",
      "\n",
      "answerLabel\tArwen\n",
      "\n",
      "\t**********\n",
      "\n"
     ]
    }
   ],
   "source": [
    "qs = ['Who directed The Shawshank Redemption?'\n",
    "     ,'Who is the director of The Shawshank Redemption?'\n",
    "      \n",
    "     ,'What is the birth date of Alan Rickman?'\n",
    "     ,'When was Alan Rickman born?'\n",
    "      \n",
    "     ,'What is the height of Amitabh Bachchan?'\n",
    "     ,'How tall is Amitabh Bachchan?'\n",
    "      \n",
    "     ,'What is the publication date of The Dark Knight?'\n",
    "     ,'When was The Dark Knight published?'\n",
    "     \n",
    "     ,'Who acted as Gollum?'\n",
    "     ,'Which actor played Gollum?'\n",
    "     \n",
    "     ,'What is the length of Interstellar?'\n",
    "     ,'How long does Interstellar run?'\n",
    "    ]\n",
    "q11 = '''When did Alan Rickman die?'''\n",
    "q12 = '''When was Pulp Fiction published?'''\n",
    "q13 = '''Where was Morgan Freeman born?'''\n",
    "q14 = '''Where does Home Alone originate?'''\n",
    "q15 = '''Which movies are directed by Alice Wu?'''\n",
    "q16 = '''How long is Pulp Fiction?'''\n",
    "q17 = '''How many episodes does Twin Peaks have?'''\n",
    "q18 = '''How long is Interstellar?'''\n",
    "q19 = '''Which character was married by Aragorn'''\n",
    "q20 = '''Which character did Aragorn marry?'''\n",
    "for i in range(12,20):\n",
    "    qs.append(globals()['q'+str(i)])\n",
    "    \n",
    "for q in qs[11:]:\n",
    "    print(f\"Query: {q}\")\n",
    "    print(pipeline(q))\n",
    "    print(\"\\t**********\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What movie won best picture in the 2020 Academy Awards?\n",
      "Answer not found\n",
      "Query: Who voices Nala in the 2019 version of The Lion King?\n",
      "here\n",
      "Who is Single Vietnam phone number\n",
      "here\n",
      "Who is Single Vietnam voice type\n",
      "here\n",
      "Who is Single Vietnam voice actor\n",
      "here\n",
      "Who is Single Vietnam audio recording of the subject's spoken voice\n",
      "here\n",
      "Nala phone number\n",
      "here\n",
      "Nala voice type\n",
      "here\n",
      "Nala voice actor\n",
      "here\n",
      "Nala audio recording of the subject's spoken voice\n",
      "here\n",
      "Nala phone number\n",
      "here\n",
      "Nala voice type\n",
      "here\n",
      "Nala voice actor\n",
      "here\n",
      "Nala audio recording of the subject's spoken voice\n",
      "Answer not found\n",
      "Query: What was the budget for Avengers: Endgame?\n",
      "Answer not found\n",
      "Query: Which is the longest Jim Carrey movie?\n",
      "Answer not found\n",
      "Query: What movie(s) feature both Jack Nicholson and Meryl Streep?\n",
      "Answer not found\n",
      "Query: Which director has been nominated most for the Best Director Academy Award?\n",
      "Answer not found\n",
      "Query: What is the name of the actor who's father was a hitman?\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Argument 'string' has incorrect type (expected str, got tuple)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-8d32bdf12ae2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mq\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mread_Qs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Query: {q[0]}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-11-d611ca2dd6cd>\u001b[0m in \u001b[0;36mpipeline\u001b[1;34m(query)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;31m# get entities\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0mentity\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_entity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[0mentity\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mentity\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"the \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mentity_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mentity_related_to_movies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_wikidata_ids\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-0d53c28cf4e4>\u001b[0m in \u001b[0;36mget_entity\u001b[1;34m(query)\u001b[0m\n\u001b[0;32m     97\u001b[0m     \u001b[0mentity\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mentity_temp\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentity_temp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m>\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mentity\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m     \u001b[0mentity_temp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_entity_complex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m     \u001b[0mentity\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mentity_temp\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentity_temp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m>\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mentity\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-0d53c28cf4e4>\u001b[0m in \u001b[0;36mget_entity_complex\u001b[1;34m(q_str)\u001b[0m\n\u001b[0;32m     29\u001b[0m     \"\"\"\n\u001b[0;32m     30\u001b[0m     \u001b[0mnlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcustom_tokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m     \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m     \u001b[1;31m# return the last named entity since the needed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[1;31m# entity is likely at the very end of the string\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Datasci\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[0;32m    981\u001b[0m         \u001b[0mDOCS\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mhttps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m//\u001b[0m\u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mapi\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[1;31m#call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    982\u001b[0m         \"\"\"\n\u001b[1;32m--> 983\u001b[1;33m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_doc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    984\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcomponent_cfg\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    985\u001b[0m             \u001b[0mcomponent_cfg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Datasci\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36mmake_doc\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m   1063\u001b[0m                 \u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE088\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1064\u001b[0m             )\n\u001b[1;32m-> 1065\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1066\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1067\u001b[0m     def update(\n",
      "\u001b[1;31mTypeError\u001b[0m: Argument 'string' has incorrect type (expected str, got tuple)"
     ]
    }
   ],
   "source": [
    "for q in read_Qs():\n",
    "    print(f\"Query: {q[0]}\")\n",
    "    print(pipeline(str(q[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pamela Anderson filmography parent astronomical body\n",
      "Pamela Anderson filmography start time\n",
      "Pamela Anderson filmography genomic start\n",
      "Pamela Anderson filmography work period (start)\n",
      "Pamela Anderson filmography cast member\n",
      "Pamela Anderson filmography number of matches played/races/starts\n",
      "Pamela Anderson filmography earliest date\n",
      "Pamela Anderson parent astronomical body\n",
      "Pamela Anderson start time\n",
      "Pamela Anderson genomic start\n",
      "Pamela Anderson work period (start)\n",
      "        entity: Pamela Anderson\n",
      "      property: work period (start)\n",
      "\n",
      "answerLabel\t1989-01-01T00:00:00Z\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(pipeline(\"Did Pamela Anderson star in Borat?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

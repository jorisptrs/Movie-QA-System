{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Analysis\n",
    "\n",
    "The goal of this assignment is to write a first version of an an interactive QA system. The system should be able to take a question in natural language (English) as input, analyse the question, and generate a SPARQL query for it.\n",
    "\n",
    "For now, we will restrict our attention to questions of the form\n",
    "\n",
    "Who/What was/is/were (the) X of Y? \n",
    "\n",
    "i.e.\n",
    "\n",
    "* What are the genres of Inception?\n",
    "* What are the main subjects of Saving Private Ryan?\n",
    "* What are the names of the Coen Brothers?\n",
    "* What is the box office of Interstellar?\n",
    "* What is the country of origin of Black Mirror?\n",
    "* What is the duration of I Am Legend?\n",
    "* What is the main subject of \"The Godfather\"?\n",
    "* Who are the founders of Pixar Animation Studios?\n",
    "* Who is the composer of Lord of The Rings?\n",
    "* etc\n",
    "\n",
    "## Interactivity\n",
    "\n",
    "In a notebook, you can ask for user input by using the input function, as shown below. The text entered by the user is stored in the variable question. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = input('Please ask a question\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linguistic Analysis with Spacy\n",
    "\n",
    "To generate a SPARQL query, the system needs to find a a property and an entity. The first step is to match the correct words in the question. So, for the first example, the property is indicated by the word _genres_ and the entity by the word _Inception_ These words can be sent as key-words to a wikidata api, to find the corresponding wikidata IDs (URIs). (More on this below)\n",
    "\n",
    "Pattern matching can be done using regular expressions (using the re python library). Alternatively, we can use a Spacy, a toolkit for doing linguistic analysis.\n",
    "\n",
    "[Spacy](https://spacy.io/usage) is a toolkit that comes with pretrained models for doing linguistic analysis in a number of languages. It can read in a text or sentence, tokenize the text (separate punctuation from words), assign Part-of-Speech (NOUN, VERB, PROPN, etc.) to tokens, lemmatize words (_actors_ --> _actor_), and detect named entities (like movie titles). See here for a short [tutorial](https://spacy.io/usage/spacy-101). \n",
    "\n",
    "When you install Spacy, make sure to also download the statistical model for analysing English sentences, en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\") # this loads the model for analysing English text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy tokenization and annotation\n",
    "\n",
    "The Spacy nlp function analyses an input text (i.e. the question of the user), and assigns an annotation to each token in the input. It returns a list of token objects, where each token object is a dictionary that has values for various attributes of each token in the sentence. \n",
    "\n",
    "The example below illustrates how to iterate over the token objects, and find interesting attributes. Spans can be useful if you want to grab multiple tokens. The analyzer also finds names of entities (persons, organisations, locations), but, unfortunately, for movie titles this often does not work. A more robust approach might be to find tokens that start with an uppercase. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'Who are the main characters of the movie Apocalypse Now?'\n",
    "\n",
    "parse = nlp(question) # parse the input \n",
    "\n",
    "for word in parse : # iterate over the token objects \n",
    "    print(word.text, word.lemma_, word.pos_)\n",
    "print(parse[3:5].text) # you can also select multiple tokens as a span. \n",
    "for ent in parse.ents : # the analysis also detects names of entities. Very unreliable for movie titles...\n",
    "    print(ent.text, ent.label_)\n",
    "for word in parse :\n",
    "        if word.text.istitle() : # check if word starts with uppercase letter \n",
    "            print(word)\n",
    "print(parse[8:10].text.istitle()) # check if all words in a span start with uppercase  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation\n",
    "\n",
    "For a quick understanding of what the parser does, and how it assigns part-of-speech, entities, etc. you can also visualise parse results. Below, the entity visualiser and parsing visualiser is demonstrated. You can ignore the arrows (dependency links) for now, we return to them next week. \n",
    "\n",
    "This code is for illustration only, it is not part of the assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "question = \"Who is the main character of the movie Harry Potter\"\n",
    "\n",
    "parse = nlp(question)\n",
    "\n",
    "displacy.render(parse, jupyter=True, style=\"ent\")\n",
    "\n",
    "displacy.render(parse, jupyter=True, style=\"dep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing the Wikidata entity finder \n",
    "\n",
    "From the parse of the user question, you can extract the words for the property and the entity that are needed to formulate a SPARQL query. The ids of the property and entity can be found by accessing the wikidata entity finder.\n",
    "\n",
    "See the example below for finding the id of the movie The Godfather. In most cases, the first result is correct, but it may be necessary to try various ids...\n",
    "\n",
    "Properties can be found by including 'type' : 'property' in the parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = 'https://www.wikidata.org/w/api.php'\n",
    "params = {'action':'wbsearchentities', \n",
    "          'language':'en',\n",
    "          'format':'json'}\n",
    "\n",
    "params['search'] = 'The Godfather'\n",
    "json = requests.get(url,params).json()\n",
    "for result in json['search']:\n",
    "    print(\"{}\\t{}\\t{}\".format(result['id'], result['label'], result['description']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a SPARQL query \n",
    "\n",
    "With the id of the property and the entity, an SPARQL query can be formulated, and the sparql endpoint of wikidata can be queried for an answer. Note that this is the same as in the previous assignment.\n",
    "\n",
    "Also note that for python, a SPARQL query is just a string, so you can construct the query by concatenating the start of the query, the ids, and the end of the query. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID1 = 'q47703'\n",
    "ID2 = 'p577'\n",
    "query = 'SELECT ?answerLabel WHERE { wd:' + ID1 + ' wdt:' + ID2 + '....'\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment \n",
    "\n",
    "Using the steps outlined above, write a function that takes input from the user, analyses it with Spacy, extracts the relevant key-words, finds the wikidata URIs for these, and sends the SPARQL query to the sparql endpoint, and prints the answer. \n",
    "\n",
    "Include 10 examples of questions that worked for your system in the comments or in a separate markdown cell. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get ID from wikidata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_wikidata_ids(name, search_property = False):\n",
    "    \"\"\"\n",
    "    Returns a list of ID dictionaries (with labels and possibly descriptions)\n",
    "    for a given name, either looking for entities or properties (set search_property:=True for the latter)\n",
    "    Each dict contains keys: 'id', 'label', and possibly 'description'.\n",
    "    If a description cannot be found, it will not be included in the dict.\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "    \n",
    "    url = 'https://www.wikidata.org/w/api.php'\n",
    "    params = {'action':'wbsearchentities', \n",
    "              'language':'en',\n",
    "              'format':'json'}\n",
    "    \n",
    "    # add a param to the request if it needs to look for a property\n",
    "    if search_property:\n",
    "        params['type'] = 'property'\n",
    "    \n",
    "    params['search'] = name\n",
    "    json = requests.get(url,params).json()\n",
    "    \n",
    "    # extract only the useful data from the json file\n",
    "    for result in json['search']:\n",
    "        # append an empty dictionary\n",
    "        all_results.append({})\n",
    "        # add the ID and label\n",
    "        all_results[-1]['id'] = result['id']\n",
    "        all_results[-1]['label'] = result['label']\n",
    "        # add a description if it exists\n",
    "        if 'description' in result.keys():\n",
    "            all_results[-1]['description'] = result['description']\n",
    "        \n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'Q13417189',\n",
       "  'label': 'Interstellar',\n",
       "  'description': '2014 British-American science fiction film directed by Christopher Nolan'},\n",
       " {'id': 'Q41872',\n",
       "  'label': 'interstellar medium',\n",
       "  'description': 'matter and radiation in the space between the star systems in a galaxy'},\n",
       " {'id': 'Q3153615',\n",
       "  'label': 'Interstellar',\n",
       "  'description': 'Wikimedia disambiguation page'},\n",
       " {'id': 'Q21186666',\n",
       "  'label': 'Interstellar',\n",
       "  'description': 'Gué Pequeno song'},\n",
       " {'id': 'Q6057099', 'label': 'Interstellar'},\n",
       " {'id': 'Q59659728', 'label': 'Interstellar', 'description': '2005 film'},\n",
       " {'id': 'Q1054444',\n",
       "  'label': 'interstellar cloud',\n",
       "  'description': 'accumulation of gas, plasma and dust in a galaxy'}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example output\n",
    "get_wikidata_ids('Interstellar', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate SPARQL Query and request results from wikidata endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sparql_query(entity_id, property_id):\n",
    "    \"\"\" \n",
    "    Returns string with entity id and property id in place as a SPARQL query\n",
    "    \"\"\"\n",
    "    query = f'''SELECT ?answerLabel WHERE {{\n",
    "                wd:{entity_id} wdt:{property_id} ?answer.\n",
    "                SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
    "                }}'''\n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests # in case a previous cell hadn't imported it already\n",
    "\n",
    "def getSPARQLresults(query):\n",
    "    \"\"\"\n",
    "    Relates to previous assignment. Return results (string) for a SPARQL query.\n",
    "    The format is arbitrary can can be changed as desired.\n",
    "    \"\"\"\n",
    "    url = 'https://query.wikidata.org/sparql'\n",
    "    results = \"\"\n",
    "    data = requests.get(url, params={'query': query, 'format': 'json'}).json()\n",
    "    for item in data['results']['bindings']:\n",
    "        for var in item :\n",
    "            results+=('{}\\t{}\\n'.format(var,item[var]['value']))\n",
    "            \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Entites and Properties from Question String\n",
    "\n",
    "A number of methods and heuristics have been implemented here to find entities and properties respectively.Some rely on assumptions or heuristics. How they work are described within the function docstring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### imports ###\n",
    "import re\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "nlp = spacy.load(\"en_core_web_sm\") # this loads the model for analysing English text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Entity extraction functions ###\n",
    "\n",
    "def get_named_entities(doc):\n",
    "    \"\"\" \n",
    "    spacy has entity recognition in-built, which might work well\n",
    "    for names, but not for multi-word named entities (like movie titles)\n",
    "    \"\"\"\n",
    "    return doc.ents\n",
    "\n",
    "def custom_tokenizer(nlp):\n",
    "    \"\"\"\n",
    "    spacy gives the programmer the ability to customize the tokenizer using regex.\n",
    "    This one specifically looks for sets of contiguous words that all have an upper-\n",
    "    case letter (i.e. that are titled). This can alternatively be done by using spacy's\n",
    "    istitle() function on all combinations of words, but that is less efficient.\n",
    "    e.g. \"How I Met Your Mother\" will be a single token using this.\n",
    "    \"\"\"\n",
    "    token_re = re.compile(r\"([A-Z][a-z']*(?:[\\s][A-Z][a-z]+)*)\")\n",
    "    return Tokenizer(nlp.vocab, token_match = token_re.findall)\n",
    "\n",
    "def get_entity_complex(q_str):\n",
    "    \"\"\"\n",
    "    calls the above function on a query string\n",
    "    \"\"\"\n",
    "    nlp.tokenizer = custom_tokenizer(nlp)\n",
    "    doc = nlp(q_str)\n",
    "    # return the last named entity since the needed \n",
    "    # entity is likely at the very end of the string\n",
    "    return doc[-1].text\n",
    "\n",
    "def regex_entity_finder(q_str):\n",
    "    \"\"\"\n",
    "    Naive regex search to find words that have capital letters.\n",
    "    The string that is returned is from the first letter of the\n",
    "    first found word, to the last letter of the last found word.\n",
    "    e.g. \"Lord of the Rings\" will be found using this.\n",
    "    \"\"\"\n",
    "    # look for concurrent titled words\n",
    "    token_re = re.compile(r\"([A-Z][a-z']*(?:[\\s][A-Z][a-z]+)*)\")\n",
    "    uppers =  [x for x in token_re.finditer(q_str)]\n",
    "    # if there are titled words, return the relevent string\n",
    "    if uppers:\n",
    "        idx1 = uppers[0].span()[0]\n",
    "        idx2 = uppers[-1].span()[1]\n",
    "        return q_str[idx1:idx2]\n",
    "    # else return an empty string\n",
    "    return \"\"\n",
    "\n",
    "def get_entity(doc):\n",
    "    \"\"\"\n",
    "    Calls upon the entities above. \n",
    "    Heuristic: a longer entity name may be more relevent\n",
    "    e.g.: \"Legend\" is less relevant than \"I Am Legend\"\n",
    "    \"\"\"\n",
    "    entity_, entity_temp = \"\", \"\"\n",
    "    \n",
    "    # if there are named entities\n",
    "    # assign entity to it\n",
    "    if get_named_entities(doc):\n",
    "        entity_ = get_named_entities(doc)[0].text\n",
    "    # check using a custom tokenizer\n",
    "    entity_temp = get_entity_complex(str(doc))\n",
    "    if len(entity_temp)>len(entity_):\n",
    "        entity_ = entity_temp\n",
    "    # check using a naive regex search\n",
    "    entity_temp = regex_entity_finder(str(doc))\n",
    "    if len(entity_temp)>len(entity_):\n",
    "        entity_ = entity_temp\n",
    "    \n",
    "    return entity_\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Property Extraction functions ###\n",
    "\n",
    "# Note: a more naive method is defined a couple of cells\n",
    "# below this one, and not included here as it does not\n",
    "# directly relate to spacy functions\n",
    "\n",
    "def get_noun_property(doc):\n",
    "    \"\"\"\n",
    "    For questions of the form given, the needed property\n",
    "    will usually be a noun that comes immediately before the\n",
    "    entity: e.g. _director_ of Shrek. For the current set of\n",
    "    questions, this will be the first noun in the string query.\n",
    "    \"\"\"\n",
    "    for word in doc : # iterate over the token objects\n",
    "        if word.pos_ == 'NOUN':\n",
    "            return(word.lemma_)\n",
    "\n",
    "def get_property(doc):\n",
    "    property_ = get_noun_property(doc)\n",
    "    return property_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Parser for natural language query ###\n",
    "\n",
    "def preprocess(query):\n",
    "    query = query.replace('?','')\n",
    "    query = query.replace(query[0], query[0].lower(), 1)\n",
    "    return query\n",
    "\n",
    "def parser(query):\n",
    "    query = preprocess(query)\n",
    "    \n",
    "    nlp = spacy.load(\"en_core_web_sm\") # this loads the model for analysing English text\n",
    "    doc = nlp(query)\n",
    "    \n",
    "    entity_ = get_entity(doc)\n",
    "    property_ = get_property(doc)\n",
    "\n",
    "    return entity_, property_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Lord of The Rings', 'composer')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example parsing: entity, property\n",
    "parser(\"Who is the composer of Lord of The Rings?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions that use major assumptions\n",
    "\n",
    "These functions are specifically designed for questions of the form *Who/What was/is/were (the) X of Y?*, and does reasonably well within the movie domain. Specifically, they are used to (i) reduce the number of queries by removing non-movie-related entities, and (ii) use a very naive method for getting a probable property from the string query for this format of question. These are not required, but may provide better quality results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Functions that are majorly hueristic/custom ###\n",
    "\n",
    "def entity_related_to_movies(entity_list):\n",
    "    \"\"\"\n",
    "    Given a list of dictionaries with information about the entity,\n",
    "    check if the description contains a word that is related to a movie.\n",
    "    These have been chosen based on wordnet's synsets. This can easily be\n",
    "    extended or made more complex using nltk, but kept straightforward for now\n",
    "    as it works well enough. This helps remove non-relevent entities that have\n",
    "    with the same name, but not related to movies (e.g. Lord of the Rings book series)\n",
    "    \"\"\"\n",
    "    valid = []\n",
    "    movie_relation = ['movie', 'film', 'picture', 'moving picture', 'motion', 'pic', 'flick', 'TV',\n",
    "                      'television', 'show', 'animation', 'animation']\n",
    "    character_relation = ['fiction', 'fictitious', 'character']\n",
    "    actor_relation = ['actor', 'actress', 'thespian']\n",
    "    for word in movie_relation+character_relation+actor_relation:\n",
    "        for e in entity_list:\n",
    "            if 'description' in e.keys():\n",
    "                if word in e['description']:\n",
    "                    if e not in valid:\n",
    "                        valid.append(e)\n",
    "                    \n",
    "    return valid\n",
    "            \n",
    "def get_property_str_naive(q_str):\n",
    "    \"\"\"\n",
    "    For the questions of the form:\n",
    "        > Who/What was/is/were the/a/an X of Y?\n",
    "    It is reasonable to assume X (the property) falls\n",
    "    squarely within the first instances of \"the\" and \"of\".\n",
    "    \"\"\"\n",
    "    the_pos = q_str.find(' the ')\n",
    "    a_pos = q_str.find(' a ')\n",
    "    an_pos = q_str.find(' an ')\n",
    "    of_pos = q_str.find(' of ')\n",
    "    # if an article and \"of\" have been found\n",
    "    # return X, else return an empty string\n",
    "    try:\n",
    "        art_pos = min([x for x in [the_pos, a_pos, an_pos] if x > 0])\n",
    "        art_len = q_str[art_pos+1:].find(' ') - art_pos\n",
    "        if art_pos != -1 and of_pos!=-1:\n",
    "            return (q_str[art_pos + art_len:of_pos].strip())\n",
    "    except:\n",
    "        pass\n",
    "    return \"\"\n",
    "\n",
    "def reduce_based_on_ids(property_list):\n",
    "    \"\"\"\n",
    "    If there are multiple ways of getting a list of properties,\n",
    "    then they may be repeated. This simply removes duplicates,\n",
    "    while not changing the relative order within the input list.\n",
    "    \"\"\"\n",
    "    p_set = {}\n",
    "    for p in property_list:\n",
    "        p_set[p['id']] = p\n",
    "\n",
    "    return list(p_set.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline: Natural Language string → SPARQL query → Results\n",
    "\n",
    "This function puts all the above functions together. It takes in a natural language query and returns the required result, if it can. The process is as such:\n",
    "\n",
    "    Input: English string of the form *Who/What was/is/were (the) X of Y?*\n",
    "    Extract entites and properties\n",
    "    Get a list of wikidata IDs that may be relevent\n",
    "        Reduce the entity ID set to only those related to movies\n",
    "        Get property from SPAQL and naive methods\n",
    "            Remove duplicate properties\n",
    "    For each entity\n",
    "          For each property\n",
    "              Generate a sparql query with (entity, property)\n",
    "              Call wikidata endpoint for results\n",
    "              If there is a result\n",
    "                  It is likely this is a relevent result\n",
    "                  so return it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def pipeline(query, moderated = False):\n",
    "    \"\"\"\n",
    "    Combines the above functions to create a pipeline to answer questions.\n",
    "    \n",
    "    Input: English string of the form \"Who/What was/is/were (the) X of Y?\"\n",
    "    Output: Result of that query if found\n",
    "    \n",
    "    Often if there are too many queries sent to the endpoint at once,\n",
    "    it will return none, so an optional boolean moderation is added\n",
    "    to add an artificial 0.5 seconds between each request. This can be slower\n",
    "    but has a lower chance of producing a request-related error. If an error\n",
    "    occurs, try running with moderated = True\n",
    "    \"\"\"\n",
    "    result = ''\n",
    "    \n",
    "    # get entities\n",
    "    entity_, property_ = parser(query)\n",
    "    entity_ = entity_.replace(\"the \", \"\")\n",
    "    entity_ids = entity_related_to_movies(get_wikidata_ids(entity_))\n",
    "    \n",
    "    # get property IDs\n",
    "    property_ids = get_wikidata_ids(property_, True)\n",
    "    # get naive property options too\n",
    "    naive_property_ = get_property_str_naive(query)\n",
    "    property_ids += get_wikidata_ids(naive_property_, True)\n",
    "    # remove duplicates\n",
    "    property_ids = reduce_based_on_ids(property_ids)\n",
    "   \n",
    "    # wrap in a try/except to help with request errors\n",
    "    try:\n",
    "        # for each combination of entities and properties\n",
    "        # it is likely that the entities and properties\n",
    "        # are sorted by relevence/similarity by wikidata\n",
    "        # so return the first result that it finds. This\n",
    "        # is not guaranteed however\n",
    "        for entity_id in entity_ids:\n",
    "            for property_id in property_ids:\n",
    "#                 print(entity_id['label'], property_id['label'])\n",
    "                # general SPARQL query\n",
    "                sparql_query = generate_sparql_query(entity_id['id'], property_id['id'])\n",
    "                result = getSPARQLresults(sparql_query)\n",
    "\n",
    "                # check if there is a result\n",
    "                if result is not None and result!='':\n",
    "                    print(\"Closest answer:\")\n",
    "                    print(f\"        entity: {entity_id['label']}\")\n",
    "                    print(f\"      property: {property_id['label']}\\n\")\n",
    "                    return result\n",
    "\n",
    "                if moderated:\n",
    "                    time.sleep(0.5)\n",
    "                    \n",
    "    except:\n",
    "        print(\"Error while searching!\")\n",
    "        if not moderated:\n",
    "            print(\"Attempting moderated search!\")\n",
    "            return pipeline(query, moderated = True)\n",
    "        \n",
    "        else:\n",
    "            pass # goes directly to final return statement\n",
    "            \n",
    "    return \"Answer not found\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closest answer:\n",
      "        entity: Shrek\n",
      "      property: child\n",
      "\n",
      "answerLabel\tFergus\n",
      "answerLabel\tFarkle\n",
      "answerLabel\tFelicia\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q = \"Who is a child of Shrek?\"\n",
    "print(pipeline(q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closest answer:\n",
      "        entity: Saving Private Ryan\n",
      "      property: main subject\n",
      "\n",
      "answerLabel\tWorld War II\n",
      "answerLabel\tInvasion of Normandy\n",
      "answerLabel\tSole Survivor Policy\n",
      "answerLabel\taltruistic suicide\n",
      "answerLabel\tOperation Overlord\n",
      "answerLabel\trescue operation\n",
      "answerLabel\tcomradeship\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q = \"What are the main subjects of Saving Private Ryan?\"\n",
    "print(pipeline(q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closest answer:\n",
      "        entity: I Am Legend\n",
      "      property: duration\n",
      "\n",
      "answerLabel\t100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q = \"What is a duration of I Am Legend?\"\n",
    "print(pipeline(q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"What is the height of Amitabh Bachchan?\"\n",
    "print(pipeline(q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closest answer:\n",
      "        entity: The Room\n",
      "      property: director\n",
      "\n",
      "answerLabel\tTommy Wiseau\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q = \"Who is the director of The Room?\"\n",
    "print(pipeline(q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"What is the birth date of Tom Hanks?\"\n",
    "print(pipeline(q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"Who is the lead actor of Johnny English?\"\n",
    "print(pipeline(q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"Who is a founder of Disney Animation?\"\n",
    "print(pipeline(q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"What is the father of Bruce Wayne?\"\n",
    "print(pipeline(q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"What is the native language of Gollum?\"\n",
    "print(pipeline(q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"What is the given names of Leonardo DiCaprio?\"\n",
    "print(pipeline(q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra question (question 11)\n",
    "# The extra heuristics are helpful for any question of\n",
    "# the given form, no matter the question itself.\n",
    "# This is a downside too, depending on the needed\n",
    "# generality of the QA system, and it might be a good\n",
    "# idea to generalise them in the future\n",
    "q = \"\"\"Who is the director of the 1997 cult space horror \n",
    "               film which also happens to be one of the films that \n",
    "               is used in this long and convoluted question, Event Horizon (1997)?\"\"\"\n",
    "print(pipeline(q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interaction is of course possible too\n",
    "q = input('Please ask a question\\n')\n",
    "print(pipeline(q))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
